{
  "accuracy_uncertainty_tradeoff": {
    "num_samples": 9,
    "counterexamples": [],
    "num_counterexamples": 0,
    "correlation": {
      "metric1": "accuracy",
      "metric2": "set_size",
      "pearson": {
        "r": -0.364267605256158,
        "p_value": 0.3351549497842576
      },
      "spearman": {
        "r": -0.2333333333333333,
        "p_value": 0.5456987782182727
      },
      "kendall": {
        "tau": -0.16666666666666666,
        "p_value": 0.612202380952381
      }
    },
    "interpretation": "Moderate negative correlation: Higher accuracy tends toward lower uncertainty"
  },
  "aggregated_results": {
    "results_matrix": {
      "('ci', 'accuracy')": {
        "smollm-135m": 0.2222222222222222,
        "smollm-360m": 0.2549019607843137,
        "tinyllama-1.1b": 0.13071895424836602
      },
      "('ci', 'set_size')": {
        "smollm-135m": 5.53921568627451,
        "smollm-360m": 5.418300653594772,
        "tinyllama-1.1b": 5.366013071895424
      },
      "('ci', 'coverage_rate')": {
        "smollm-135m": 0.9477124183006534,
        "smollm-360m": 0.9575163398692811,
        "tinyllama-1.1b": 0.9575163398692811
      },
      "('qa', 'accuracy')": {
        "smollm-135m": 0.16339869281045752,
        "smollm-360m": 0.18954248366013074,
        "tinyllama-1.1b": 0.23529411764705885
      },
      "('qa', 'set_size')": {
        "smollm-135m": 5.669934640522875,
        "smollm-360m": 5.545751633986928,
        "tinyllama-1.1b": 5.718954248366013
      },
      "('qa', 'coverage_rate')": {
        "smollm-135m": 0.9607843137254902,
        "smollm-360m": 0.9346405228758169,
        "tinyllama-1.1b": 0.9640522875816994
      },
      "('rc', 'accuracy')": {
        "smollm-135m": 0.1503267973856209,
        "smollm-360m": 0.2745098039215686,
        "tinyllama-1.1b": 0.30718954248366015
      },
      "('rc', 'set_size')": {
        "smollm-135m": 5.5130718954248366,
        "smollm-360m": 5.2026143790849675,
        "tinyllama-1.1b": 5.366013071895426
      },
      "('rc', 'coverage_rate')": {
        "smollm-135m": 0.9248366013071897,
        "smollm-360m": 0.9117647058823529,
        "tinyllama-1.1b": 0.9313725490196078
      }
    },
    "model_rankings": {
      "smollm-360m": 1,
      "tinyllama-1.1b": 2,
      "smollm-135m": 3
    },
    "task_rankings": {
      "ci": {
        "smollm-360m": 1,
        "smollm-135m": 2,
        "tinyllama-1.1b": 3
      },
      "qa": {
        "tinyllama-1.1b": 1,
        "smollm-360m": 2,
        "smollm-135m": 3
      },
      "rc": {
        "tinyllama-1.1b": 1,
        "smollm-360m": 2,
        "smollm-135m": 3
      }
    },
    "summary_stats": {
      "accuracy": {
        "mean": 0.21423384168482207,
        "std": 0.05645836376347657,
        "min": 0.13071895424836602,
        "max": 0.30718954248366015
      },
      "set_size": {
        "mean": 5.482207697893973,
        "std": 0.15231166579943517,
        "min": 5.2026143790849675,
        "max": 5.718954248366013
      },
      "coverage_rate": {
        "mean": 0.9433551198257081,
        "std": 0.017360977614933805,
        "min": 0.9117647058823529,
        "max": 0.9640522875816994
      },
      "num_models": 3,
      "num_tasks": 3,
      "total_evaluations": 9
    },
    "best_overall": "smollm-360m",
    "best_per_task": {
      "ci": "smollm-360m",
      "qa": "tinyllama-1.1b",
      "rc": "tinyllama-1.1b"
    }
  }
}