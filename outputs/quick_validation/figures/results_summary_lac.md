# BLUQ Benchmark Results - LAC

## Accuracy (%)

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Dialogue Response | Document Summarization | Avg |
|---|---|---|---|---|---|---|
| phi-2 | 0.3 | 0.3 | 0.2 | - | 0.3 | **0.3** |
| stablelm-2-1.6b | 0.2 | 0.2 | 0.3 | 0.1 | 0.2 | **0.2** |
| tinyllama-1.1b | 0.2 | 0.1 | 0.2 | 0.2 | 0.1 | **0.2** |

## Coverage Rate (%)

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Dialogue Response | Document Summarization | Avg |
|---|---|---|---|---|---|---|
| phi-2 | 1.0 | 0.9 | 0.9 | - | 1.0 | **0.9** |
| stablelm-2-1.6b | 0.9 | 0.8 | 0.9 | 0.9 | 1.0 | **0.9** |
| tinyllama-1.1b | 1.0 | 0.7 | 1.0 | 0.9 | 1.0 | **0.9** |

## Average Set Size

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Dialogue Response | Document Summarization | Avg |
|---|---|---|---|---|---|---|
| phi-2 | 5.61 | 5.39 | 5.39 | - | 5.47 | **5.47** |
| stablelm-2-1.6b | 5.29 | 4.67 | 5.65 | 5.53 | 5.20 | **5.27** |
| tinyllama-1.1b | 5.69 | 4.55 | 5.43 | 5.20 | 5.25 | **5.22** |