# BLUQ Benchmark Results

## Accuracy (%)

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Dialogue Response | Document Summarization | Avg |
|---|---|---|---|---|---|---|
| phi-2 | 0.3 | 0.2 | 0.3 | 0.3 | 0.3 | **0.3** |
| stablelm-2-1.6b | 0.2 | 0.2 | 0.3 | 0.1 | 0.2 | **0.2** |
| tinyllama-1.1b | 0.2 | 0.1 | 0.2 | 0.2 | 0.1 | **0.2** |

## Coverage Rate (%)

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Dialogue Response | Document Summarization | Avg |
|---|---|---|---|---|---|---|
| phi-2 | 1.0 | 0.9 | 1.0 | 0.9 | 1.0 | **1.0** |
| stablelm-2-1.6b | 0.9 | 0.9 | 1.0 | 1.0 | 1.0 | **0.9** |
| tinyllama-1.1b | 1.0 | 0.9 | 1.0 | 0.9 | 1.0 | **1.0** |

## Average Set Size

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Dialogue Response | Document Summarization | Avg |
|---|---|---|---|---|---|---|
| phi-2 | 5.95 | 5.41 | 5.71 | 5.58 | 5.73 | **5.67** |
| stablelm-2-1.6b | 5.49 | 5.23 | 5.70 | 5.76 | 5.42 | **5.52** |
| tinyllama-1.1b | 5.82 | 5.45 | 5.63 | 5.47 | 5.31 | **5.54** |