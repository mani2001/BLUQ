2025-12-07 23:15:26 - __main__ - INFO - Starting benchmark run
2025-12-07 23:15:26 - __main__ - INFO - Log file: outputs/results/logs/benchmark_20251207_231526.log
2025-12-07 23:15:26 - __main__ - INFO - Mode: long, Samples: 10000
2025-12-07 23:15:26 - __main__ - INFO - Models: ['phi-2']
2025-12-07 23:15:26 - __main__ - INFO - Tasks: ['qa', 'rc', 'ci', 'drs', 'ds']
2025-12-07 23:15:26 - __main__ - INFO - Dtypes: ['float16']
2025-12-07 23:15:27 - __main__ - INFO - Running on: Device: NVIDIA A100 80GB PCIe (cuda)
  Total Memory: 79.25 GB
  Available Memory: 79.25 GB
2025-12-07 23:15:27 - src.utils.gpu - INFO - Detected A100 GPU: NVIDIA A100 80GB PCIe - using optimized A100 settings
2025-12-07 23:15:27 - __main__ - INFO - GPU Tier: a100 | Auto max_batch_size: 192 | Safety margin: 0.93
2025-12-07 23:15:27 - __main__ - INFO - Auto-detected max_batch_size: 192
2025-12-07 23:15:27 - src.utils.gpu - INFO - Detected A100 GPU: NVIDIA A100 80GB PCIe - using optimized A100 settings
2025-12-07 23:15:27 - src.utils.gpu - INFO - Initialized GPUMemoryManager:
Device: NVIDIA A100 80GB PCIe (cuda)
  Total Memory: 79.25 GB
  Available Memory: 79.25 GB
  GPU Tier: a100
  Safety Margin: 0.93
  Activation Multiplier: 0.6
  Max Batch Size: 192
2025-12-07 23:15:27 - src.utils.gpu - INFO - NVML initialized for GPU utilization monitoring
2025-12-07 23:15:27 - src.utils.gpu - INFO - GPUProfiler initialized (CUDA: True, NVML: True)
2025-12-07 23:15:27 - src.utils.gpu - INFO - Started GPU monitoring
2025-12-07 23:15:27 - __main__ - INFO - GPU profiling enabled
2025-12-07 23:15:27 - __main__ - INFO - 
================================================================================
2025-12-07 23:15:27 - __main__ - INFO - FULL BENCHMARK CONFIGURATION
2025-12-07 23:15:27 - __main__ - INFO - ================================================================================
2025-12-07 23:15:27 - __main__ - INFO - Models (1): ['phi-2']
2025-12-07 23:15:27 - __main__ - INFO - Tasks (5): ['qa', 'rc', 'ci', 'drs', 'ds']
2025-12-07 23:15:27 - __main__ - INFO - Data types: ['float16']
2025-12-07 23:15:27 - __main__ - INFO - Samples per task: 10000
2025-12-07 23:15:27 - __main__ - INFO - Alpha (error rate): 0.1
2025-12-07 23:15:27 - __main__ - INFO - Strategies: ['base']
2025-12-07 23:15:27 - __main__ - INFO - Conformal methods: ['lac', 'aps']
2025-12-07 23:15:27 - __main__ - INFO - Dynamic batch sizing: True
2025-12-07 23:15:27 - __main__ - INFO - Max batch size: 192 (GPU tier: a100)
2025-12-07 23:15:27 - __main__ - INFO - Output directory: outputs/results
2025-12-07 23:15:27 - __main__ - INFO - 
================================================================================
2025-12-07 23:15:27 - __main__ - INFO - Run 1/5: phi-2 | qa | float16
2025-12-07 23:15:27 - __main__ - INFO - ================================================================================
2025-12-07 23:15:31 - src.utils.gpu - INFO - [start_phi-2_qa] GPU State: Allocated: 0.0MB | Reserved: 0.0MB | Free: 81155.8MB | Utilization: 0.0%
2025-12-07 23:15:31 - __main__ - INFO - Loading qa dataset (10000 samples)...
2025-12-07 23:15:31 - src.data.dataset_loader - INFO - Loading MMLU dataset with 10000 samples...
2025-12-07 23:15:36 - src.data.dataset_loader - INFO - Loaded 10000 MMLU instances
2025-12-07 23:15:36 - src.utils.gpu - INFO - [dataset_loading] Duration: 4999.06ms | GPU Memory: 0.0MB -> 0.0MB (delta +0.0MB)
2025-12-07 23:15:36 - src.data.dataset_processor - INFO - Processing qa dataset to 6-option format...
2025-12-07 23:15:36 - src.data.dataset_processor - INFO - Processed 10000 instances for qa
2025-12-07 23:15:36 - src.data.data_splitter - INFO - Splitting qa dataset (calibration: 50%, test: 50%)
2025-12-07 23:15:36 - src.data.data_splitter - INFO - Split complete:
2025-12-07 23:15:36 - src.data.data_splitter - INFO -   Calibration: 4998 instances
2025-12-07 23:15:36 - src.data.data_splitter - INFO -   Test: 5002 instances
2025-12-07 23:15:36 - src.data.data_splitter - WARNING - Calibration size 4998 differs from expected 5000
2025-12-07 23:15:36 - src.data.data_splitter - INFO - Answer distribution:
2025-12-07 23:15:36 - src.data.data_splitter - INFO -   Calibration: {'A': 1174, 'B': 1230, 'C': 1242, 'D': 1352}
2025-12-07 23:15:36 - src.data.data_splitter - INFO -   Test: {'A': 1175, 'B': 1231, 'C': 1243, 'D': 1353}
2025-12-07 23:15:36 - src.utils.gpu - INFO - [dataset_processing] Duration: 206.53ms | GPU Memory: 0.0MB -> 0.0MB (delta +0.0MB)
2025-12-07 23:15:36 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationSelector
2025-12-07 23:15:36 - src.prompting.demonstration_manager - INFO -   Strategy: random
2025-12-07 23:15:36 - src.prompting.demonstration_manager - INFO -   Num demonstrations: 5
2025-12-07 23:15:36 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationManager
2025-12-07 23:15:36 - src.prompting.demonstration_manager - INFO - Selecting 5 demonstrations using 'random' strategy
2025-12-07 23:15:36 - src.prompting.demonstration_manager - INFO - Selected and cached 5 demonstrations for qa
2025-12-07 23:15:36 - __main__ - INFO - Loading model: phi-2 (float16)
2025-12-07 23:15:36 - src.models.model_loader - INFO - Loading model: phi-2_float16 (microsoft/phi-2)
2025-12-07 23:15:36 - src.models.model_loader - INFO - Loading tokenizer from microsoft/phi-2
2025-12-07 23:15:36 - src.models.model_loader - INFO - Tokenizer loaded successfully
2025-12-07 23:15:36 - src.models.model_loader - INFO -   Vocab size: 50295
2025-12-07 23:15:36 - src.models.model_loader - INFO -   Padding side: left
2025-12-07 23:15:36 - src.models.model_loader - INFO -   PAD token: <|endoftext|> (ID: 50256)
2025-12-07 23:15:36 - src.models.model_loader - INFO - Loading model from microsoft/phi-2
2025-12-07 23:15:45 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-12-07 23:15:47 - src.models.model_loader - INFO - Model loaded successfully
2025-12-07 23:15:47 - src.models.model_loader - INFO -   GPU Memory: 5.18GB allocated, 5.18GB reserved
2025-12-07 23:15:47 - src.utils.gpu - INFO - [model_loading] Duration: 10994.89ms | GPU Memory: 0.0MB -> 5301.8MB (delta +5301.8MB)
2025-12-07 23:15:47 - src.utils.gpu - INFO - [after_model_load_phi-2] GPU State: Allocated: 5301.8MB | Reserved: 5308.0MB | Free: 75853.9MB | Utilization: 0.0%
2025-12-07 23:15:47 - src.utils.gpu - INFO - Optimal batch size for 2.7B model (float16): 13
  Available memory: 73.71GB
  Model memory: 5.40GB
  Memory per batch item: 5.092GB
  Vocab multiplier: 1.57x (vocab_size=50295)
2025-12-07 23:15:47 - __main__ - INFO - Using batch size: 13
2025-12-07 23:15:47 - __main__ - INFO -   Strategy: base
2025-12-07 23:15:47 - src.prompting.prompt_builder - INFO - Initialized PromptBuilder for task: qa
2025-12-07 23:15:47 - src.prompting.prompt_builder - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
2025-12-07 23:15:47 - src.prompting.prompt_builder - INFO - Building 10000 prompts using 'base' strategy with 5 demonstrations
2025-12-07 23:15:47 - src.models.inference_engine - INFO - Initialized InferenceEngine for phi-2_float16
2025-12-07 23:15:47 - src.models.inference_engine - INFO -   Device: cuda:0
2025-12-07 23:15:47 - src.models.inference_engine - INFO -   Batch size: 13
2025-12-07 23:15:47 - src.models.inference_engine - INFO -   Option tokens: {'A': 317, 'B': 347, 'C': 327, 'D': 360, 'E': 412, 'F': 376}
