2025-12-08 01:01:40 - __main__ - INFO - Starting benchmark run
2025-12-08 01:01:40 - __main__ - INFO - Log file: outputs/results/logs/benchmark_20251208_010140.log
2025-12-08 01:01:40 - __main__ - INFO - Mode: long, Samples: 10000
2025-12-08 01:01:40 - __main__ - INFO - Models: ['gemma-2b-it']
2025-12-08 01:01:40 - __main__ - INFO - Tasks: ['qa', 'rc', 'ci', 'drs', 'ds']
2025-12-08 01:01:40 - __main__ - INFO - Dtypes: ['float16']
2025-12-08 01:01:42 - __main__ - INFO - Running on: Device: NVIDIA A100 80GB PCIe (cuda)
  Total Memory: 79.25 GB
  Available Memory: 79.25 GB
2025-12-08 01:01:42 - src.utils.gpu - INFO - Detected A100 GPU: NVIDIA A100 80GB PCIe - using optimized A100 settings
2025-12-08 01:01:42 - __main__ - INFO - GPU Tier: a100 | Auto max_batch_size: 192 | Safety margin: 0.93
2025-12-08 01:01:42 - __main__ - INFO - Using user-specified max_batch_size: 64
2025-12-08 01:01:42 - src.utils.gpu - INFO - NVML initialized for GPU utilization monitoring
2025-12-08 01:01:42 - src.utils.gpu - INFO - GPUProfiler initialized (CUDA: True, NVML: True)
2025-12-08 01:01:42 - src.utils.gpu - INFO - Started GPU monitoring
2025-12-08 01:01:42 - __main__ - INFO - GPU profiling enabled
2025-12-08 01:01:42 - __main__ - INFO - 
================================================================================
2025-12-08 01:01:42 - __main__ - INFO - FULL BENCHMARK CONFIGURATION
2025-12-08 01:01:42 - __main__ - INFO - ================================================================================
2025-12-08 01:01:42 - __main__ - INFO - Models (1): ['gemma-2b-it']
2025-12-08 01:01:42 - __main__ - INFO - Tasks (5): ['qa', 'rc', 'ci', 'drs', 'ds']
2025-12-08 01:01:42 - __main__ - INFO - Data types: ['float16']
2025-12-08 01:01:42 - __main__ - INFO - Samples per task: 10000
2025-12-08 01:01:42 - __main__ - INFO - Alpha (error rate): 0.1
2025-12-08 01:01:42 - __main__ - INFO - Strategies: ['base']
2025-12-08 01:01:42 - __main__ - INFO - Conformal methods: ['lac', 'aps']
2025-12-08 01:01:42 - __main__ - INFO - Dynamic batch sizing: False
2025-12-08 01:01:42 - __main__ - INFO - Max batch size: 64 (GPU tier: a100)
2025-12-08 01:01:42 - __main__ - INFO - Output directory: outputs/results
2025-12-08 01:01:42 - __main__ - INFO - 
================================================================================
2025-12-08 01:01:42 - __main__ - INFO - Run 1/5: gemma-2b-it | qa | float16
2025-12-08 01:01:42 - __main__ - INFO - ================================================================================
2025-12-08 01:01:46 - src.utils.gpu - INFO - [start_gemma-2b-it_qa] GPU State: Allocated: 0.0MB | Reserved: 0.0MB | Free: 81155.8MB | Utilization: 0.0%
2025-12-08 01:01:46 - __main__ - INFO - Loading qa dataset (10000 samples)...
2025-12-08 01:01:46 - src.data.dataset_loader - INFO - Loading MMLU dataset with 10000 samples...
2025-12-08 01:01:50 - src.data.dataset_loader - INFO - Loaded 10000 MMLU instances
2025-12-08 01:01:50 - src.utils.gpu - INFO - [dataset_loading] Duration: 4489.94ms | GPU Memory: 0.0MB -> 0.0MB (delta +0.0MB)
2025-12-08 01:01:50 - src.data.dataset_processor - INFO - Processing qa dataset to 6-option format...
2025-12-08 01:01:51 - src.data.dataset_processor - INFO - Processed 10000 instances for qa
2025-12-08 01:01:51 - src.data.data_splitter - INFO - Splitting qa dataset (calibration: 50%, test: 50%)
2025-12-08 01:01:51 - src.data.data_splitter - INFO - Split complete:
2025-12-08 01:01:51 - src.data.data_splitter - INFO -   Calibration: 4998 instances
2025-12-08 01:01:51 - src.data.data_splitter - INFO -   Test: 5002 instances
2025-12-08 01:01:51 - src.data.data_splitter - WARNING - Calibration size 4998 differs from expected 5000
2025-12-08 01:01:51 - src.data.data_splitter - INFO - Answer distribution:
2025-12-08 01:01:51 - src.data.data_splitter - INFO -   Calibration: {'A': 1174, 'B': 1230, 'C': 1242, 'D': 1352}
2025-12-08 01:01:51 - src.data.data_splitter - INFO -   Test: {'A': 1175, 'B': 1231, 'C': 1243, 'D': 1353}
2025-12-08 01:01:51 - src.utils.gpu - INFO - [dataset_processing] Duration: 236.45ms | GPU Memory: 0.0MB -> 0.0MB (delta +0.0MB)
2025-12-08 01:01:51 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationSelector
2025-12-08 01:01:51 - src.prompting.demonstration_manager - INFO -   Strategy: random
2025-12-08 01:01:51 - src.prompting.demonstration_manager - INFO -   Num demonstrations: 5
2025-12-08 01:01:51 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationManager
2025-12-08 01:01:51 - src.prompting.demonstration_manager - INFO - Selecting 5 demonstrations using 'random' strategy
2025-12-08 01:01:51 - src.prompting.demonstration_manager - INFO - Selected and cached 5 demonstrations for qa
2025-12-08 01:01:51 - __main__ - INFO - Loading model: gemma-2b-it (float16)
2025-12-08 01:01:51 - src.models.model_loader - INFO - Loading model: gemma-2b-it_float16 (google/gemma-2b-it)
2025-12-08 01:01:51 - src.models.model_loader - INFO - Loading tokenizer from google/gemma-2b-it
2025-12-08 01:01:53 - src.models.model_loader - INFO - Tokenizer loaded successfully
2025-12-08 01:01:53 - src.models.model_loader - INFO -   Vocab size: 256000
2025-12-08 01:01:53 - src.models.model_loader - INFO -   Padding side: left
2025-12-08 01:01:53 - src.models.model_loader - INFO -   PAD token: <pad> (ID: 0)
2025-12-08 01:01:53 - src.models.model_loader - INFO - Loading model from google/gemma-2b-it
2025-12-08 01:02:01 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-12-08 01:02:02 - src.models.model_loader - INFO - Model loaded successfully
2025-12-08 01:02:02 - src.models.model_loader - INFO -   GPU Memory: 4.67GB allocated, 4.77GB reserved
2025-12-08 01:02:03 - src.utils.gpu - INFO - [model_loading] Duration: 11924.50ms | GPU Memory: 0.0MB -> 4780.1MB (delta +4780.1MB)
2025-12-08 01:02:03 - src.utils.gpu - INFO - [after_model_load_gemma-2b-it] GPU State: Allocated: 4780.1MB | Reserved: 4882.0MB | Free: 76375.6MB | Utilization: 2.0%
2025-12-08 01:02:03 - __main__ - INFO - Using batch size: 64
2025-12-08 01:02:03 - __main__ - INFO -   Strategy: base
2025-12-08 01:02:03 - src.prompting.prompt_builder - INFO - Initialized PromptBuilder for task: qa
2025-12-08 01:02:03 - src.prompting.prompt_builder - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
2025-12-08 01:02:03 - src.prompting.prompt_builder - INFO - Building 10000 prompts using 'base' strategy with 5 demonstrations
2025-12-08 01:02:03 - src.models.inference_engine - INFO - Initialized InferenceEngine for gemma-2b-it_float16
2025-12-08 01:02:03 - src.models.inference_engine - INFO -   Device: cuda:0
2025-12-08 01:02:03 - src.models.inference_engine - INFO -   Batch size: 64
2025-12-08 01:02:03 - src.models.inference_engine - INFO -   Option tokens: {'A': 586, 'B': 599, 'C': 585, 'D': 608, 'E': 637, 'F': 633}
2025-12-08 01:02:15 - src.utils.gpu - INFO - [inference_calibration] Duration: 12568.23ms | GPU Memory: 4780.1MB -> 5146.9MB (delta +366.8MB)
2025-12-08 01:02:15 - __main__ - ERROR - Failed: gemma-2b-it | qa | float16
2025-12-08 01:02:15 - __main__ - ERROR - Error: CUDA out of memory. Tried to allocate 43.61 GiB. GPU 0 has a total capacity of 79.25 GiB of which 39.40 GiB is free. Process 1776638 has 39.84 GiB memory in use. Of the allocated memory 5.03 GiB is allocated by PyTorch, and 34.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/BLUQ/run_benchmark.py", line 337, in run
    results = self._run_single_configuration(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/run_benchmark.py", line 535, in _run_single_configuration
    cal_results = inference_engine.infer_batch(cal_prompts, show_progress=True)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/src/models/inference_engine.py", line 392, in infer_batch
    batch_results = self._infer_batch_internal(batch_prompts, batch_ids)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/src/models/inference_engine.py", line 434, in _infer_batch_internal
    outputs = self.model(**inputs, use_cache=use_cache)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/transformers/models/gemma/modeling_gemma.py", line 482, in forward
    logits = self.lm_head(hidden_states[:, slice_indices, :])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 43.61 GiB. GPU 0 has a total capacity of 79.25 GiB of which 39.40 GiB is free. Process 1776638 has 39.84 GiB memory in use. Of the allocated memory 5.03 GiB is allocated by PyTorch, and 34.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-08 01:02:15 - __main__ - INFO - 
================================================================================
2025-12-08 01:02:15 - __main__ - INFO - Run 2/5: gemma-2b-it | rc | float16
2025-12-08 01:02:15 - __main__ - INFO - ================================================================================
2025-12-08 01:02:15 - src.utils.gpu - INFO - [start_gemma-2b-it_rc] GPU State: Allocated: 8.1MB | Reserved: 40290.0MB | Free: 81147.6MB | Utilization: 64.0%
2025-12-08 01:02:15 - __main__ - INFO - Loading rc dataset (10000 samples)...
2025-12-08 01:02:15 - src.data.dataset_loader - INFO - Loading CosmosQA dataset with 10000 samples...
2025-12-08 01:02:17 - src.data.dataset_loader - INFO - CosmosQA loaded successfully with standard method
2025-12-08 01:02:18 - src.data.dataset_loader - INFO - Loaded 10000 CosmosQA instances
2025-12-08 01:02:18 - src.utils.gpu - INFO - [dataset_loading] Duration: 2592.30ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
2025-12-08 01:02:18 - src.data.dataset_processor - INFO - Processing rc dataset to 6-option format...
2025-12-08 01:02:18 - src.data.dataset_processor - INFO - Processed 10000 instances for rc
2025-12-08 01:02:18 - src.data.data_splitter - INFO - Splitting rc dataset (calibration: 50%, test: 50%)
2025-12-08 01:02:18 - src.data.data_splitter - INFO - Split complete:
2025-12-08 01:02:18 - src.data.data_splitter - INFO -   Calibration: 4999 instances
2025-12-08 01:02:18 - src.data.data_splitter - INFO -   Test: 5001 instances
2025-12-08 01:02:18 - src.data.data_splitter - INFO - Answer distribution:
2025-12-08 01:02:18 - src.data.data_splitter - INFO -   Calibration: {'A': 1240, 'B': 1245, 'C': 1263, 'D': 1251}
2025-12-08 01:02:18 - src.data.data_splitter - INFO -   Test: {'A': 1241, 'B': 1245, 'C': 1264, 'D': 1251}
2025-12-08 01:02:18 - src.utils.gpu - INFO - [dataset_processing] Duration: 36.28ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
2025-12-08 01:02:18 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationSelector
2025-12-08 01:02:18 - src.prompting.demonstration_manager - INFO -   Strategy: random
2025-12-08 01:02:18 - src.prompting.demonstration_manager - INFO -   Num demonstrations: 5
2025-12-08 01:02:18 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationManager
2025-12-08 01:02:18 - src.prompting.demonstration_manager - INFO - Selecting 5 demonstrations using 'random' strategy
2025-12-08 01:02:18 - src.prompting.demonstration_manager - INFO - Selected and cached 5 demonstrations for rc
2025-12-08 01:02:18 - __main__ - INFO - Loading model: gemma-2b-it (float16)
2025-12-08 01:02:18 - src.models.model_loader - INFO - Loading model: gemma-2b-it_float16 (google/gemma-2b-it)
2025-12-08 01:02:18 - src.models.model_loader - INFO - Loading tokenizer from google/gemma-2b-it
2025-12-08 01:02:19 - src.models.model_loader - INFO - Tokenizer loaded successfully
2025-12-08 01:02:19 - src.models.model_loader - INFO -   Vocab size: 256000
2025-12-08 01:02:19 - src.models.model_loader - INFO -   Padding side: left
2025-12-08 01:02:19 - src.models.model_loader - INFO -   PAD token: <pad> (ID: 0)
2025-12-08 01:02:19 - src.models.model_loader - INFO - Loading model from google/gemma-2b-it
2025-12-08 01:02:19 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-12-08 01:02:21 - src.models.model_loader - INFO - Model loaded successfully
2025-12-08 01:02:21 - src.models.model_loader - INFO -   GPU Memory: 4.68GB allocated, 39.35GB reserved
2025-12-08 01:02:21 - src.utils.gpu - INFO - [model_loading] Duration: 2605.83ms | GPU Memory: 8.1MB -> 4788.3MB (delta +4780.1MB)
2025-12-08 01:02:21 - src.utils.gpu - INFO - [after_model_load_gemma-2b-it] GPU State: Allocated: 4788.3MB | Reserved: 40290.0MB | Free: 76367.5MB | Utilization: 0.0%
2025-12-08 01:02:21 - __main__ - INFO - Using batch size: 64
2025-12-08 01:02:21 - __main__ - INFO -   Strategy: base
2025-12-08 01:02:21 - src.prompting.prompt_builder - INFO - Initialized PromptBuilder for task: rc
2025-12-08 01:02:21 - src.prompting.prompt_builder - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
2025-12-08 01:02:21 - src.prompting.prompt_builder - INFO - Building 10000 prompts using 'base' strategy with 5 demonstrations
2025-12-08 01:02:21 - src.models.inference_engine - INFO - Initialized InferenceEngine for gemma-2b-it_float16
2025-12-08 01:02:21 - src.models.inference_engine - INFO -   Device: cuda:0
2025-12-08 01:02:21 - src.models.inference_engine - INFO -   Batch size: 64
2025-12-08 01:02:21 - src.models.inference_engine - INFO -   Option tokens: {'A': 586, 'B': 599, 'C': 585, 'D': 608, 'E': 637, 'F': 633}
2025-12-08 01:05:10 - src.utils.gpu - INFO - [inference_calibration] Duration: 168867.27ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
2025-12-08 01:08:00 - src.utils.gpu - INFO - [inference_test] Duration: 170285.22ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
2025-12-08 01:08:00 - __main__ - INFO -     Inference: 339.15s for 10000 samples (29.49 samples/sec)
2025-12-08 01:08:00 - src.models.probability_extractor - INFO - Initialized ProbabilityExtractor
2025-12-08 01:08:00 - src.models.probability_extractor - INFO -   Temperature: 1.0
2025-12-08 01:08:00 - src.models.probability_extractor - INFO -   Calibration method: None
2025-12-08 01:08:00 - src.utils.gpu - INFO - [probability_extraction] Duration: 273.99ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
2025-12-08 01:08:00 - __main__ - INFO -     Raw probabilities saved to: outputs/results/probabilities/probs_gemma-2b-it_rc_float16_base_20251208_010140.npz
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO - Initialized LACScorer
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Alpha: 0.1
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Target coverage: 90.0%
2025-12-08 01:08:00 - src.conformal.scorers - INFO - Initialized LAC (Least Ambiguous set-valued Classifiers) scorer
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO - Initialized PredictionSetGenerator
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO -   Methods: ['lac']
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO -   Alpha: 0.1
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO -   Aggregation: separate
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO - Calibrating 1 conformal predictors...
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO -   Calibrating LAC...
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO - Calibrating with 4999 samples...
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO - Calibration complete
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Threshold: 0.9958
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Score range: [0.0000, 1.0000]
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO - Calibration complete
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO - Generating prediction sets using LAC...
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO - Generating prediction sets for 5001 test instances...
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO - Prediction complete
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Average set size: 5.27
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Coverage rate: 90.40%
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Meets coverage guarantee: True
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO - [PASS] Coverage guarantee met: 90.40% >= 90.00%
2025-12-08 01:08:00 - __main__ - INFO -     LAC: Acc=22.86%, CR=90.40%, SS=5.27
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO - Initialized APSScorer
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Alpha: 0.1
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Target coverage: 90.0%
2025-12-08 01:08:00 - src.conformal.scorers - INFO - Initialized APS (Adaptive Prediction Sets) scorer
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO - Initialized PredictionSetGenerator
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO -   Methods: ['aps']
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO -   Alpha: 0.1
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO -   Aggregation: separate
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO - Calibrating 1 conformal predictors...
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO -   Calibrating APS...
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO - Calibrating with 4999 samples...
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO - Calibration complete
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Threshold: 1.0000
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO -   Score range: [0.2081, 1.0000]
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO - Calibration complete
2025-12-08 01:08:00 - src.conformal.prediction_set_generator - INFO - Generating prediction sets using APS...
2025-12-08 01:08:00 - src.conformal.conformal_base - INFO - Generating prediction sets for 5001 test instances...
2025-12-08 01:08:01 - src.conformal.conformal_base - INFO - Prediction complete
2025-12-08 01:08:01 - src.conformal.conformal_base - INFO -   Average set size: 5.17
2025-12-08 01:08:01 - src.conformal.conformal_base - INFO -   Coverage rate: 90.66%
2025-12-08 01:08:01 - src.conformal.conformal_base - INFO -   Meets coverage guarantee: True
2025-12-08 01:08:01 - src.conformal.conformal_base - INFO - [PASS] Coverage guarantee met: 90.66% >= 90.00%
2025-12-08 01:08:01 - __main__ - INFO -     APS: Acc=22.86%, CR=90.66%, SS=5.17
2025-12-08 01:08:05 - src.models.model_loader - INFO - Unloaded model: gemma-2b-it_float16
2025-12-08 01:08:06 - __main__ - INFO - Checkpoint saved after completing: gemma-2b-it | rc | float16
2025-12-08 01:08:06 - __main__ - INFO - 
================================================================================
2025-12-08 01:08:06 - __main__ - INFO - Run 3/5: gemma-2b-it | ci | float16
2025-12-08 01:08:06 - __main__ - INFO - ================================================================================
2025-12-08 01:08:06 - src.utils.gpu - INFO - [start_gemma-2b-it_ci] GPU State: Allocated: 8.1MB | Reserved: 4780.0MB | Free: 81147.6MB | Utilization: 100.0%
2025-12-08 01:08:06 - __main__ - INFO - Loading ci dataset (10000 samples)...
2025-12-08 01:08:06 - src.data.dataset_loader - INFO - Loading HellaSwag dataset with 10000 samples...
2025-12-08 01:08:10 - src.data.dataset_loader - INFO - Loaded 10000 HellaSwag instances
2025-12-08 01:08:10 - src.utils.gpu - INFO - [dataset_loading] Duration: 4010.28ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
2025-12-08 01:08:10 - src.data.dataset_processor - INFO - Processing ci dataset to 6-option format...
2025-12-08 01:08:10 - src.data.dataset_processor - INFO - Processed 10000 instances for ci
2025-12-08 01:08:10 - src.data.data_splitter - INFO - Splitting ci dataset (calibration: 50%, test: 50%)
2025-12-08 01:08:10 - src.data.data_splitter - INFO - Split complete:
2025-12-08 01:08:10 - src.data.data_splitter - INFO -   Calibration: 4999 instances
2025-12-08 01:08:10 - src.data.data_splitter - INFO -   Test: 5001 instances
2025-12-08 01:08:10 - src.data.data_splitter - INFO - Answer distribution:
2025-12-08 01:08:10 - src.data.data_splitter - INFO -   Calibration: {'A': 1240, 'B': 1242, 'C': 1256, 'D': 1261}
2025-12-08 01:08:10 - src.data.data_splitter - INFO -   Test: {'A': 1240, 'B': 1242, 'C': 1257, 'D': 1262}
2025-12-08 01:08:10 - src.utils.gpu - INFO - [dataset_processing] Duration: 41.93ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
2025-12-08 01:08:10 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationSelector
2025-12-08 01:08:10 - src.prompting.demonstration_manager - INFO -   Strategy: random
2025-12-08 01:08:10 - src.prompting.demonstration_manager - INFO -   Num demonstrations: 5
2025-12-08 01:08:10 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationManager
2025-12-08 01:08:10 - src.prompting.demonstration_manager - INFO - Selecting 5 demonstrations using 'random' strategy
2025-12-08 01:08:10 - src.prompting.demonstration_manager - INFO - Selected and cached 5 demonstrations for ci
2025-12-08 01:08:10 - __main__ - INFO - Loading model: gemma-2b-it (float16)
2025-12-08 01:08:10 - src.models.model_loader - INFO - Loading model: gemma-2b-it_float16 (google/gemma-2b-it)
2025-12-08 01:08:10 - src.models.model_loader - INFO - Loading tokenizer from google/gemma-2b-it
2025-12-08 01:08:11 - src.models.model_loader - INFO - Tokenizer loaded successfully
2025-12-08 01:08:11 - src.models.model_loader - INFO -   Vocab size: 256000
2025-12-08 01:08:11 - src.models.model_loader - INFO -   Padding side: left
2025-12-08 01:08:11 - src.models.model_loader - INFO -   PAD token: <pad> (ID: 0)
2025-12-08 01:08:11 - src.models.model_loader - INFO - Loading model from google/gemma-2b-it
2025-12-08 01:08:11 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-12-08 01:08:12 - src.models.model_loader - INFO - Model loaded successfully
2025-12-08 01:08:12 - src.models.model_loader - INFO -   GPU Memory: 4.68GB allocated, 4.77GB reserved
2025-12-08 01:08:12 - src.utils.gpu - INFO - [model_loading] Duration: 2525.51ms | GPU Memory: 8.1MB -> 4788.3MB (delta +4780.1MB)
2025-12-08 01:08:12 - src.utils.gpu - INFO - [after_model_load_gemma-2b-it] GPU State: Allocated: 4788.3MB | Reserved: 4882.0MB | Free: 76367.5MB | Utilization: 1.0%
2025-12-08 01:08:12 - __main__ - INFO - Using batch size: 64
2025-12-08 01:08:12 - __main__ - INFO -   Strategy: base
2025-12-08 01:08:12 - src.prompting.prompt_builder - INFO - Initialized PromptBuilder for task: ci
2025-12-08 01:08:12 - src.prompting.prompt_builder - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
2025-12-08 01:08:12 - src.prompting.prompt_builder - INFO - Building 10000 prompts using 'base' strategy with 5 demonstrations
2025-12-08 01:08:12 - src.models.inference_engine - INFO - Initialized InferenceEngine for gemma-2b-it_float16
2025-12-08 01:08:12 - src.models.inference_engine - INFO -   Device: cuda:0
2025-12-08 01:08:12 - src.models.inference_engine - INFO -   Batch size: 64
2025-12-08 01:08:12 - src.models.inference_engine - INFO -   Option tokens: {'A': 586, 'B': 599, 'C': 585, 'D': 608, 'E': 637, 'F': 633}
2025-12-08 01:11:00 - src.utils.gpu - INFO - [inference_calibration] Duration: 167995.01ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
2025-12-08 01:13:50 - src.utils.gpu - INFO - [inference_test] Duration: 169352.01ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
2025-12-08 01:13:50 - __main__ - INFO -     Inference: 337.35s for 10000 samples (29.64 samples/sec)
2025-12-08 01:13:50 - src.models.probability_extractor - INFO - Initialized ProbabilityExtractor
2025-12-08 01:13:50 - src.models.probability_extractor - INFO -   Temperature: 1.0
2025-12-08 01:13:50 - src.models.probability_extractor - INFO -   Calibration method: None
2025-12-08 01:13:50 - src.utils.gpu - INFO - [probability_extraction] Duration: 292.58ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
2025-12-08 01:13:50 - __main__ - INFO -     Raw probabilities saved to: outputs/results/probabilities/probs_gemma-2b-it_ci_float16_base_20251208_010140.npz
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - Initialized LACScorer
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Alpha: 0.1
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Target coverage: 90.0%
2025-12-08 01:13:50 - src.conformal.scorers - INFO - Initialized LAC (Least Ambiguous set-valued Classifiers) scorer
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO - Initialized PredictionSetGenerator
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO -   Methods: ['lac']
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO -   Alpha: 0.1
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO -   Aggregation: separate
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO - Calibrating 1 conformal predictors...
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO -   Calibrating LAC...
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - Calibrating with 4999 samples...
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - Calibration complete
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Threshold: 0.9965
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Score range: [0.0000, 1.0000]
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO - Calibration complete
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO - Generating prediction sets using LAC...
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - Generating prediction sets for 5001 test instances...
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - Prediction complete
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Average set size: 5.32
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Coverage rate: 90.44%
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Meets coverage guarantee: True
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - [PASS] Coverage guarantee met: 90.44% >= 90.00%
2025-12-08 01:13:50 - __main__ - INFO -     LAC: Acc=22.08%, CR=90.44%, SS=5.32
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - Initialized APSScorer
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Alpha: 0.1
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Target coverage: 90.0%
2025-12-08 01:13:50 - src.conformal.scorers - INFO - Initialized APS (Adaptive Prediction Sets) scorer
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO - Initialized PredictionSetGenerator
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO -   Methods: ['aps']
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO -   Alpha: 0.1
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO -   Aggregation: separate
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO - Calibrating 1 conformal predictors...
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO -   Calibrating APS...
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - Calibrating with 4999 samples...
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - Calibration complete
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Threshold: 1.0000
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Score range: [0.2352, 1.0000]
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO - Calibration complete
2025-12-08 01:13:50 - src.conformal.prediction_set_generator - INFO - Generating prediction sets using APS...
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - Generating prediction sets for 5001 test instances...
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - Prediction complete
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Average set size: 5.86
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Coverage rate: 98.54%
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO -   Meets coverage guarantee: True
2025-12-08 01:13:50 - src.conformal.conformal_base - INFO - [PASS] Coverage guarantee met: 98.54% >= 90.00%
2025-12-08 01:13:50 - __main__ - INFO -     APS: Acc=22.08%, CR=98.54%, SS=5.86
2025-12-08 01:13:55 - src.models.model_loader - INFO - Unloaded model: gemma-2b-it_float16
2025-12-08 01:13:55 - __main__ - INFO - Checkpoint saved after completing: gemma-2b-it | ci | float16
2025-12-08 01:13:55 - __main__ - INFO - 
================================================================================
2025-12-08 01:13:55 - __main__ - INFO - Run 4/5: gemma-2b-it | drs | float16
2025-12-08 01:13:55 - __main__ - INFO - ================================================================================
2025-12-08 01:13:55 - src.utils.gpu - INFO - [start_gemma-2b-it_drs] GPU State: Allocated: 8.1MB | Reserved: 4780.0MB | Free: 81147.6MB | Utilization: 100.0%
2025-12-08 01:13:55 - __main__ - INFO - Loading drs dataset (10000 samples)...
2025-12-08 01:13:55 - src.data.dataset_loader - INFO - Loading HaluDial dataset with 10000 samples...
2025-12-08 01:13:58 - src.data.dataset_loader - INFO - Loaded 10000 HaluDial instances
2025-12-08 01:13:58 - src.utils.gpu - INFO - [dataset_loading] Duration: 2131.13ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
2025-12-08 01:13:58 - src.data.dataset_processor - INFO - Processing drs dataset to 6-option format...
2025-12-08 01:14:31 - src.data.dataset_processor - INFO - Processed 10000 instances for drs
2025-12-08 01:14:31 - src.data.dataset_processor - INFO - Option expansion statistics for drs:
2025-12-08 01:14:31 - src.data.dataset_processor - INFO -   Instances expanded (2→4 options): 10000
2025-12-08 01:14:31 - src.data.dataset_processor - INFO -   Total options sampled: 20000
2025-12-08 01:14:31 - src.data.dataset_processor - INFO -   Duplicate options avoided: 2
2025-12-08 01:14:31 - src.data.dataset_processor - INFO -   Fallback options used: 0
2025-12-08 01:14:31 - src.data.data_splitter - INFO - Splitting drs dataset (calibration: 50%, test: 50%)
2025-12-08 01:14:31 - src.data.data_splitter - INFO - Split complete:
2025-12-08 01:14:31 - src.data.data_splitter - INFO -   Calibration: 4999 instances
2025-12-08 01:14:31 - src.data.data_splitter - INFO -   Test: 5001 instances
2025-12-08 01:14:31 - src.data.data_splitter - INFO - Answer distribution:
2025-12-08 01:14:31 - src.data.data_splitter - INFO -   Calibration: {'A': 1258, 'B': 1203, 'C': 1264, 'D': 1274}
2025-12-08 01:14:31 - src.data.data_splitter - INFO -   Test: {'A': 1258, 'B': 1204, 'C': 1265, 'D': 1274}
2025-12-08 01:14:31 - src.utils.gpu - INFO - [dataset_processing] Duration: 33612.38ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
2025-12-08 01:14:31 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationSelector
2025-12-08 01:14:31 - src.prompting.demonstration_manager - INFO -   Strategy: random
2025-12-08 01:14:31 - src.prompting.demonstration_manager - INFO -   Num demonstrations: 5
2025-12-08 01:14:31 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationManager
2025-12-08 01:14:31 - src.prompting.demonstration_manager - INFO - Selecting 3 demonstrations using 'random' strategy
2025-12-08 01:14:31 - src.prompting.demonstration_manager - INFO - Selected and cached 3 demonstrations for drs
2025-12-08 01:14:31 - __main__ - INFO - Loading model: gemma-2b-it (float16)
2025-12-08 01:14:31 - src.models.model_loader - INFO - Loading model: gemma-2b-it_float16 (google/gemma-2b-it)
2025-12-08 01:14:31 - src.models.model_loader - INFO - Loading tokenizer from google/gemma-2b-it
2025-12-08 01:14:32 - src.models.model_loader - INFO - Tokenizer loaded successfully
2025-12-08 01:14:32 - src.models.model_loader - INFO -   Vocab size: 256000
2025-12-08 01:14:32 - src.models.model_loader - INFO -   Padding side: left
2025-12-08 01:14:32 - src.models.model_loader - INFO -   PAD token: <pad> (ID: 0)
2025-12-08 01:14:32 - src.models.model_loader - INFO - Loading model from google/gemma-2b-it
2025-12-08 01:14:32 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-12-08 01:14:34 - src.models.model_loader - INFO - Model loaded successfully
2025-12-08 01:14:34 - src.models.model_loader - INFO -   GPU Memory: 4.68GB allocated, 4.77GB reserved
2025-12-08 01:14:34 - src.utils.gpu - INFO - [model_loading] Duration: 2533.22ms | GPU Memory: 8.1MB -> 4788.3MB (delta +4780.1MB)
2025-12-08 01:14:34 - src.utils.gpu - INFO - [after_model_load_gemma-2b-it] GPU State: Allocated: 4788.3MB | Reserved: 4882.0MB | Free: 76367.5MB | Utilization: 2.0%
2025-12-08 01:14:34 - __main__ - INFO - Using batch size: 64
2025-12-08 01:14:34 - __main__ - INFO -   Strategy: base
2025-12-08 01:14:34 - src.prompting.prompt_builder - INFO - Initialized PromptBuilder for task: drs
2025-12-08 01:14:34 - src.prompting.prompt_builder - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
2025-12-08 01:14:34 - src.prompting.prompt_builder - INFO - Building 10000 prompts using 'base' strategy with 3 demonstrations
2025-12-08 01:14:34 - src.models.inference_engine - INFO - Initialized InferenceEngine for gemma-2b-it_float16
2025-12-08 01:14:34 - src.models.inference_engine - INFO -   Device: cuda:0
2025-12-08 01:14:34 - src.models.inference_engine - INFO -   Batch size: 64
2025-12-08 01:14:34 - src.models.inference_engine - INFO -   Option tokens: {'A': 586, 'B': 599, 'C': 585, 'D': 608, 'E': 637, 'F': 633}
2025-12-08 01:16:19 - src.utils.gpu - INFO - [inference_calibration] Duration: 105303.69ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
2025-12-08 01:18:06 - src.utils.gpu - INFO - [inference_test] Duration: 107170.67ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
2025-12-08 01:18:06 - __main__ - INFO -     Inference: 212.48s for 10000 samples (47.06 samples/sec)
2025-12-08 01:18:06 - src.models.probability_extractor - INFO - Initialized ProbabilityExtractor
2025-12-08 01:18:06 - src.models.probability_extractor - INFO -   Temperature: 1.0
2025-12-08 01:18:06 - src.models.probability_extractor - INFO -   Calibration method: None
2025-12-08 01:18:07 - src.utils.gpu - INFO - [probability_extraction] Duration: 274.55ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
2025-12-08 01:18:07 - __main__ - INFO -     Raw probabilities saved to: outputs/results/probabilities/probs_gemma-2b-it_drs_float16_base_20251208_010140.npz
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - Initialized LACScorer
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Alpha: 0.1
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Target coverage: 90.0%
2025-12-08 01:18:07 - src.conformal.scorers - INFO - Initialized LAC (Least Ambiguous set-valued Classifiers) scorer
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO - Initialized PredictionSetGenerator
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO -   Methods: ['lac']
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO -   Alpha: 0.1
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO -   Aggregation: separate
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO - Calibrating 1 conformal predictors...
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO -   Calibrating LAC...
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - Calibrating with 4999 samples...
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - Calibration complete
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Threshold: 0.9966
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Score range: [0.0000, 1.0000]
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO - Calibration complete
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO - Generating prediction sets using LAC...
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - Generating prediction sets for 5001 test instances...
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - Prediction complete
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Average set size: 5.26
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Coverage rate: 90.24%
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Meets coverage guarantee: True
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - [PASS] Coverage guarantee met: 90.24% >= 90.00%
2025-12-08 01:18:07 - __main__ - INFO -     LAC: Acc=22.80%, CR=90.24%, SS=5.26
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - Initialized APSScorer
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Alpha: 0.1
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Target coverage: 90.0%
2025-12-08 01:18:07 - src.conformal.scorers - INFO - Initialized APS (Adaptive Prediction Sets) scorer
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO - Initialized PredictionSetGenerator
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO -   Methods: ['aps']
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO -   Alpha: 0.1
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO -   Aggregation: separate
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO - Calibrating 1 conformal predictors...
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO -   Calibrating APS...
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - Calibrating with 4999 samples...
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - Calibration complete
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Threshold: 1.0000
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Score range: [0.2331, 1.0000]
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO - Calibration complete
2025-12-08 01:18:07 - src.conformal.prediction_set_generator - INFO - Generating prediction sets using APS...
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - Generating prediction sets for 5001 test instances...
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - Prediction complete
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Average set size: 5.16
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Coverage rate: 90.80%
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO -   Meets coverage guarantee: True
2025-12-08 01:18:07 - src.conformal.conformal_base - INFO - [PASS] Coverage guarantee met: 90.80% >= 90.00%
2025-12-08 01:18:07 - __main__ - INFO -     APS: Acc=22.80%, CR=90.80%, SS=5.16
2025-12-08 01:18:11 - src.models.model_loader - INFO - Unloaded model: gemma-2b-it_float16
2025-12-08 01:18:12 - __main__ - INFO - Checkpoint saved after completing: gemma-2b-it | drs | float16
2025-12-08 01:18:12 - __main__ - INFO - 
================================================================================
2025-12-08 01:18:12 - __main__ - INFO - Run 5/5: gemma-2b-it | ds | float16
2025-12-08 01:18:12 - __main__ - INFO - ================================================================================
2025-12-08 01:18:12 - src.utils.gpu - INFO - [start_gemma-2b-it_ds] GPU State: Allocated: 8.1MB | Reserved: 4780.0MB | Free: 81147.6MB | Utilization: 100.0%
2025-12-08 01:18:12 - __main__ - INFO - Loading ds dataset (10000 samples)...
2025-12-08 01:18:12 - src.data.dataset_loader - INFO - Loading HaluSum dataset with 10000 samples...
2025-12-08 01:18:13 - src.data.dataset_loader - INFO - Loaded 10000 HaluSum instances
2025-12-08 01:18:13 - src.utils.gpu - INFO - [dataset_loading] Duration: 1745.87ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
2025-12-08 01:18:13 - src.data.dataset_processor - INFO - Processing ds dataset to 6-option format...
2025-12-08 01:18:49 - src.data.dataset_processor - INFO - Processed 10000 instances for ds
2025-12-08 01:18:49 - src.data.dataset_processor - INFO - Option expansion statistics for ds:
2025-12-08 01:18:49 - src.data.dataset_processor - INFO -   Instances expanded (2→4 options): 10000
2025-12-08 01:18:49 - src.data.dataset_processor - INFO -   Total options sampled: 20000
2025-12-08 01:18:49 - src.data.dataset_processor - INFO -   Duplicate options avoided: 0
2025-12-08 01:18:49 - src.data.dataset_processor - INFO -   Fallback options used: 0
2025-12-08 01:18:49 - src.data.data_splitter - INFO - Splitting ds dataset (calibration: 50%, test: 50%)
2025-12-08 01:18:49 - src.data.data_splitter - INFO - Split complete:
2025-12-08 01:18:49 - src.data.data_splitter - INFO -   Calibration: 4999 instances
2025-12-08 01:18:49 - src.data.data_splitter - INFO -   Test: 5001 instances
2025-12-08 01:18:49 - src.data.data_splitter - INFO - Answer distribution:
2025-12-08 01:18:49 - src.data.data_splitter - INFO -   Calibration: {'A': 1258, 'B': 1203, 'C': 1264, 'D': 1274}
2025-12-08 01:18:49 - src.data.data_splitter - INFO -   Test: {'A': 1258, 'B': 1204, 'C': 1265, 'D': 1274}
2025-12-08 01:18:49 - src.utils.gpu - INFO - [dataset_processing] Duration: 35766.75ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
2025-12-08 01:18:49 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationSelector
2025-12-08 01:18:49 - src.prompting.demonstration_manager - INFO -   Strategy: random
2025-12-08 01:18:49 - src.prompting.demonstration_manager - INFO -   Num demonstrations: 5
2025-12-08 01:18:49 - src.prompting.demonstration_manager - INFO - Initialized DemonstrationManager
2025-12-08 01:18:49 - src.prompting.demonstration_manager - INFO - Selecting 1 demonstrations using 'random' strategy
2025-12-08 01:18:49 - src.prompting.demonstration_manager - INFO - Selected and cached 1 demonstrations for ds
2025-12-08 01:18:49 - __main__ - INFO - Loading model: gemma-2b-it (float16)
2025-12-08 01:18:49 - src.models.model_loader - INFO - Loading model: gemma-2b-it_float16 (google/gemma-2b-it)
2025-12-08 01:18:49 - src.models.model_loader - INFO - Loading tokenizer from google/gemma-2b-it
2025-12-08 01:18:50 - src.models.model_loader - INFO - Tokenizer loaded successfully
2025-12-08 01:18:50 - src.models.model_loader - INFO -   Vocab size: 256000
2025-12-08 01:18:50 - src.models.model_loader - INFO -   Padding side: left
2025-12-08 01:18:50 - src.models.model_loader - INFO -   PAD token: <pad> (ID: 0)
2025-12-08 01:18:50 - src.models.model_loader - INFO - Loading model from google/gemma-2b-it
2025-12-08 01:18:50 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-12-08 01:18:52 - src.models.model_loader - INFO - Model loaded successfully
2025-12-08 01:18:52 - src.models.model_loader - INFO -   GPU Memory: 4.68GB allocated, 4.77GB reserved
2025-12-08 01:18:52 - src.utils.gpu - INFO - [model_loading] Duration: 2758.07ms | GPU Memory: 8.1MB -> 4788.3MB (delta +4780.1MB)
2025-12-08 01:18:52 - src.utils.gpu - INFO - [after_model_load_gemma-2b-it] GPU State: Allocated: 4788.3MB | Reserved: 4882.0MB | Free: 76367.5MB | Utilization: 2.0%
2025-12-08 01:18:52 - __main__ - INFO - Using batch size: 64
2025-12-08 01:18:52 - __main__ - INFO -   Strategy: base
2025-12-08 01:18:52 - src.prompting.prompt_builder - INFO - Initialized PromptBuilder for task: ds
2025-12-08 01:18:52 - src.prompting.prompt_builder - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
2025-12-08 01:18:52 - src.prompting.prompt_builder - INFO - Building 10000 prompts using 'base' strategy with 1 demonstrations
2025-12-08 01:18:52 - src.models.inference_engine - INFO - Initialized InferenceEngine for gemma-2b-it_float16
2025-12-08 01:18:52 - src.models.inference_engine - INFO -   Device: cuda:0
2025-12-08 01:18:52 - src.models.inference_engine - INFO -   Batch size: 64
2025-12-08 01:18:52 - src.models.inference_engine - INFO -   Option tokens: {'A': 586, 'B': 599, 'C': 585, 'D': 608, 'E': 637, 'F': 633}
2025-12-08 01:19:01 - src.utils.gpu - INFO - [inference_calibration] Duration: 9031.99ms | GPU Memory: 4788.3MB -> 5302.3MB (delta +514.0MB)
2025-12-08 01:19:01 - __main__ - ERROR - Failed: gemma-2b-it | ds | float16
2025-12-08 01:19:01 - __main__ - ERROR - Error: CUDA out of memory. Tried to allocate 62.50 GiB. GPU 0 has a total capacity of 79.25 GiB of which 11.48 GiB is free. Process 1776638 has 67.77 GiB memory in use. Of the allocated memory 5.18 GiB is allocated by PyTorch, and 62.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/BLUQ/run_benchmark.py", line 337, in run
    results = self._run_single_configuration(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/run_benchmark.py", line 535, in _run_single_configuration
    cal_results = inference_engine.infer_batch(cal_prompts, show_progress=True)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/src/models/inference_engine.py", line 392, in infer_batch
    batch_results = self._infer_batch_internal(batch_prompts, batch_ids)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/src/models/inference_engine.py", line 434, in _infer_batch_internal
    outputs = self.model(**inputs, use_cache=use_cache)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/transformers/models/gemma/modeling_gemma.py", line 482, in forward
    logits = self.lm_head(hidden_states[:, slice_indices, :])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 62.50 GiB. GPU 0 has a total capacity of 79.25 GiB of which 11.48 GiB is free. Process 1776638 has 67.77 GiB memory in use. Of the allocated memory 5.18 GiB is allocated by PyTorch, and 62.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-08 01:19:01 - src.visualization.result_visualizer - INFO - Generating all visualizations...
2025-12-08 01:19:02 - src.visualization.result_visualizer - INFO - Saved heatmap to outputs/results/figures/heatmap_accuracy.png
2025-12-08 01:19:02 - src.visualization.result_visualizer - INFO - Saved heatmap to outputs/results/figures/heatmap_coverage_rate.png
2025-12-08 01:19:03 - src.visualization.result_visualizer - INFO - Saved heatmap to outputs/results/figures/heatmap_avg_set_size.png
2025-12-08 01:19:04 - src.visualization.result_visualizer - INFO - Saved dashboard to outputs/results/figures/dashboard.png
2025-12-08 01:19:05 - src.visualization.result_visualizer - INFO - Saved bar chart to outputs/results/figures/bar_comparison_accuracy.png
2025-12-08 01:19:05 - src.visualization.result_visualizer - INFO - Saved bar chart to outputs/results/figures/bar_comparison_avg_set_size.png
2025-12-08 01:19:06 - src.visualization.result_visualizer - INFO - Saved radar chart to outputs/results/figures/radar_chart.png
2025-12-08 01:19:06 - src.visualization.result_visualizer - INFO - Saved uncertainty analysis to outputs/results/figures/uncertainty_analysis.png
2025-12-08 01:19:06 - src.visualization.result_visualizer - INFO - Saved summary table to outputs/results/figures/results_summary.md
2025-12-08 01:19:06 - src.visualization.result_visualizer - INFO - Saved summary table to outputs/results/figures/results_summary_lac.md
2025-12-08 01:19:06 - src.visualization.result_visualizer - INFO - Saved summary table to outputs/results/figures/results_summary_aps.md
2025-12-08 01:19:06 - src.visualization.result_visualizer - INFO - All visualizations saved to outputs/results/figures
2025-12-08 01:19:06 - __main__ - INFO - Visualizations saved to: outputs/results/figures
2025-12-08 01:19:07 - src.utils.gpu - INFO - Stopped GPU monitoring. Collected 462 snapshots.
2025-12-08 01:19:07 - src.utils.gpu - INFO - Saved GPU profiling report to outputs/results/gpu_profile_20251208_010140.json
2025-12-08 01:19:07 - __main__ - INFO - GPU profiling report saved to: outputs/results/gpu_profile_20251208_010140.json
2025-12-08 01:19:07 - __main__ - INFO - 
================================================================================
2025-12-08 01:19:07 - __main__ - INFO - BENCHMARK COMPLETE
2025-12-08 01:19:07 - __main__ - INFO - ================================================================================
2025-12-08 01:19:07 - __main__ - INFO - Total time: 17.32 minutes
2025-12-08 01:19:07 - __main__ - INFO - Results saved to: outputs/results
2025-12-08 01:19:07 - __main__ - INFO - Benchmark completed successfully
2025-12-08 01:19:07 - __main__ - INFO - Total runs: 6
2025-12-08 01:19:07 - __main__ - INFO - Overall accuracy: 22.58%
2025-12-08 01:19:07 - __main__ - INFO - Overall coverage: 91.85%
2025-12-08 01:19:07 - __main__ - INFO - Log file saved to: outputs/results/logs/benchmark_20251208_010140.log
