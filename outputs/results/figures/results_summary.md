# BLUQ Benchmark Results

## Accuracy (%)

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Avg |
|---|---|---|---|---|
| phi-2 | 0.3 | 0.3 | 0.2 | **0.3** |
| tinyllama-1.1b | 0.2 | 0.3 | 0.2 | **0.2** |

## Coverage Rate (%)

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Avg |
|---|---|---|---|---|
| phi-2 | 1.0 | 0.9 | 1.0 | **1.0** |
| tinyllama-1.1b | 1.0 | 0.9 | 0.9 | **0.9** |

## Average Set Size

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Avg |
|---|---|---|---|---|
| phi-2 | 5.77 | 5.35 | 5.79 | **5.64** |
| tinyllama-1.1b | 5.49 | 5.30 | 5.22 | **5.34** |