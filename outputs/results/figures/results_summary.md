# BLUQ Benchmark Results

## Accuracy (%)

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Dialogue Response | Document Summarization | Avg |
|---|---|---|---|---|---|---|
| phi-2 | 0.2 | 0.3 | 0.2 | 0.2 | 0.4 | **0.3** |

## Coverage Rate (%)

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Dialogue Response | Document Summarization | Avg |
|---|---|---|---|---|---|---|
| phi-2 | 0.9 | 1.0 | 0.9 | 0.9 | 0.9 | **0.9** |

## Average Set Size

| Model | Question Answering | Reading Comprehension | Commonsense Inference | Dialogue Response | Document Summarization | Avg |
|---|---|---|---|---|---|---|
| phi-2 | 5.60 | 5.58 | 5.52 | 5.59 | 5.31 | **5.52** |