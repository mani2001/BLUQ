2025-12-08 01:01:40 - __main__ - WARNING - Model 'gemma-2b-it' not in predefined list, attempting anyway...

================================================================================
BLUQ: Benchmarking Language models via Uncertainty Quantification
================================================================================
Mode: long (10000 samples per task)
Models: 1 | Tasks: 5 | Dtypes: 1
Total configurations: 5
Log file: outputs/results/logs/benchmark_20251208_010140.log
================================================================================

01:01:40 - INFO - Starting benchmark run
01:01:40 - INFO - Log file: outputs/results/logs/benchmark_20251208_010140.log
01:01:40 - INFO - Mode: long, Samples: 10000
01:01:40 - INFO - Models: ['gemma-2b-it']
01:01:40 - INFO - Tasks: ['qa', 'rc', 'ci', 'drs', 'ds']
01:01:40 - INFO - Dtypes: ['float16']
01:01:42 - INFO - Running on: Device: NVIDIA A100 80GB PCIe (cuda)
  Total Memory: 79.25 GB
  Available Memory: 79.25 GB
01:01:42 - INFO - Detected A100 GPU: NVIDIA A100 80GB PCIe - using optimized A100 settings
01:01:42 - INFO - GPU Tier: a100 | Auto max_batch_size: 192 | Safety margin: 0.93
01:01:42 - INFO - Using user-specified max_batch_size: 64
01:01:42 - INFO - NVML initialized for GPU utilization monitoring
01:01:42 - INFO - GPUProfiler initialized (CUDA: True, NVML: True)
01:01:42 - INFO - Started GPU monitoring
01:01:42 - INFO - GPU profiling enabled
01:01:42 - INFO - 
================================================================================
01:01:42 - INFO - FULL BENCHMARK CONFIGURATION
01:01:42 - INFO - ================================================================================
01:01:42 - INFO - Models (1): ['gemma-2b-it']
01:01:42 - INFO - Tasks (5): ['qa', 'rc', 'ci', 'drs', 'ds']
01:01:42 - INFO - Data types: ['float16']
01:01:42 - INFO - Samples per task: 10000
01:01:42 - INFO - Alpha (error rate): 0.1
01:01:42 - INFO - Strategies: ['base']
01:01:42 - INFO - Conformal methods: ['lac', 'aps']
01:01:42 - INFO - Dynamic batch sizing: False
01:01:42 - INFO - Max batch size: 64 (GPU tier: a100)
01:01:42 - INFO - Output directory: outputs/results
01:01:42 - INFO - 
================================================================================
01:01:42 - INFO - Run 1/5: gemma-2b-it | qa | float16
01:01:42 - INFO - ================================================================================
01:01:46 - INFO - [start_gemma-2b-it_qa] GPU State: Allocated: 0.0MB | Reserved: 0.0MB | Free: 81155.8MB | Utilization: 0.0%
01:01:46 - INFO - Loading qa dataset (10000 samples)...
01:01:46 - INFO - Loading MMLU dataset with 10000 samples...
Downloading readme: 0.00B [00:00, ?B/s]Downloading readme: 53.2kB [00:00, 81.3MB/s]
Downloading metadata: 0.00B [00:00, ?B/s]Downloading metadata: 138kB [00:00, 85.5MB/s]
01:01:50 - INFO - Loaded 10000 MMLU instances
01:01:50 - INFO - [dataset_loading] Duration: 4489.94ms | GPU Memory: 0.0MB -> 0.0MB (delta +0.0MB)
01:01:50 - INFO - Processing qa dataset to 6-option format...
01:01:51 - INFO - Processed 10000 instances for qa
01:01:51 - INFO - Splitting qa dataset (calibration: 50%, test: 50%)
01:01:51 - INFO - Split complete:
01:01:51 - INFO -   Calibration: 4998 instances
01:01:51 - INFO -   Test: 5002 instances
01:01:51 - WARNING - Calibration size 4998 differs from expected 5000
01:01:51 - INFO - Answer distribution:
01:01:51 - INFO -   Calibration: {'A': 1174, 'B': 1230, 'C': 1242, 'D': 1352}
01:01:51 - INFO -   Test: {'A': 1175, 'B': 1231, 'C': 1243, 'D': 1353}
01:01:51 - INFO - [dataset_processing] Duration: 236.45ms | GPU Memory: 0.0MB -> 0.0MB (delta +0.0MB)
01:01:51 - INFO - Initialized DemonstrationSelector
01:01:51 - INFO -   Strategy: random
01:01:51 - INFO -   Num demonstrations: 5
01:01:51 - INFO - Initialized DemonstrationManager
01:01:51 - INFO - Selecting 5 demonstrations using 'random' strategy
01:01:51 - INFO - Selected and cached 5 demonstrations for qa
01:01:51 - INFO - Loading model: gemma-2b-it (float16)
01:01:51 - INFO - Loading model: gemma-2b-it_float16 (google/gemma-2b-it)
01:01:51 - INFO - Loading tokenizer from google/gemma-2b-it
01:01:53 - INFO - Tokenizer loaded successfully
01:01:53 - INFO -   Vocab size: 256000
01:01:53 - INFO -   Padding side: left
01:01:53 - INFO -   PAD token: <pad> (ID: 0)
01:01:53 - INFO - Loading model from google/gemma-2b-it
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files:  50%|█████     | 1/2 [00:07<00:07,  7.11s/it]Fetching 2 files: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]
01:02:01 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
01:02:02 - INFO - Model loaded successfully
01:02:02 - INFO -   GPU Memory: 4.67GB allocated, 4.77GB reserved

Model: gemma-2b-it_float16
  ID: google/gemma-2b-it
  Type: causal
  Parameters: 2,506,172,416
  Vocab size: 256,000
  Max length: 8192
  Device: cuda:0
  Dtype: torch.float16
  Instruct-tuned: True
01:02:03 - INFO - [model_loading] Duration: 11924.50ms | GPU Memory: 0.0MB -> 4780.1MB (delta +4780.1MB)
01:02:03 - INFO - [after_model_load_gemma-2b-it] GPU State: Allocated: 4780.1MB | Reserved: 4882.0MB | Free: 76375.6MB | Utilization: 2.0%
01:02:03 - INFO - Using batch size: 64
01:02:03 - INFO -   Strategy: base
01:02:03 - INFO - Initialized PromptBuilder for task: qa
01:02:03 - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
01:02:03 - INFO - Building 10000 prompts using 'base' strategy with 5 demonstrations
01:02:03 - INFO - Initialized InferenceEngine for gemma-2b-it_float16
01:02:03 - INFO -   Device: cuda:0
01:02:03 - INFO -   Batch size: 64
01:02:03 - INFO -   Option tokens: {'A': 586, 'B': 599, 'C': 585, 'D': 608, 'E': 637, 'F': 633}
Running inference:   0%|          | 0/79 [00:00<?, ?it/s]/root/BLUQ/src/models/inference_engine.py:433: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Running inference:   1%|▏         | 1/79 [00:02<03:35,  2.76s/it]Running inference:   3%|▎         | 2/79 [00:06<04:06,  3.20s/it]Running inference:   4%|▍         | 3/79 [00:08<03:42,  2.93s/it]Running inference:   4%|▍         | 3/79 [00:12<05:18,  4.19s/it]
01:02:15 - INFO - [inference_calibration] Duration: 12568.23ms | GPU Memory: 4780.1MB -> 5146.9MB (delta +366.8MB)
01:02:15 - ERROR - Failed: gemma-2b-it | qa | float16
01:02:15 - ERROR - Error: CUDA out of memory. Tried to allocate 43.61 GiB. GPU 0 has a total capacity of 79.25 GiB of which 39.40 GiB is free. Process 1776638 has 39.84 GiB memory in use. Of the allocated memory 5.03 GiB is allocated by PyTorch, and 34.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/BLUQ/run_benchmark.py", line 337, in run
    results = self._run_single_configuration(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/run_benchmark.py", line 535, in _run_single_configuration
    cal_results = inference_engine.infer_batch(cal_prompts, show_progress=True)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/src/models/inference_engine.py", line 392, in infer_batch
    batch_results = self._infer_batch_internal(batch_prompts, batch_ids)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/src/models/inference_engine.py", line 434, in _infer_batch_internal
    outputs = self.model(**inputs, use_cache=use_cache)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/transformers/models/gemma/modeling_gemma.py", line 482, in forward
    logits = self.lm_head(hidden_states[:, slice_indices, :])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 43.61 GiB. GPU 0 has a total capacity of 79.25 GiB of which 39.40 GiB is free. Process 1776638 has 39.84 GiB memory in use. Of the allocated memory 5.03 GiB is allocated by PyTorch, and 34.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
01:02:15 - INFO - 
================================================================================
01:02:15 - INFO - Run 2/5: gemma-2b-it | rc | float16
01:02:15 - INFO - ================================================================================
01:02:15 - INFO - [start_gemma-2b-it_rc] GPU State: Allocated: 8.1MB | Reserved: 40290.0MB | Free: 81147.6MB | Utilization: 64.0%
01:02:15 - INFO - Loading rc dataset (10000 samples)...
01:02:15 - INFO - Loading CosmosQA dataset with 10000 samples...
Downloading builder script: 0.00B [00:00, ?B/s]Downloading builder script: 4.32kB [00:00, 16.2MB/s]
Downloading readme: 0.00B [00:00, ?B/s]Downloading readme: 7.51kB [00:00, 25.9MB/s]
01:02:17 - INFO - CosmosQA loaded successfully with standard method
01:02:18 - INFO - Loaded 10000 CosmosQA instances
01:02:18 - INFO - [dataset_loading] Duration: 2592.30ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
01:02:18 - INFO - Processing rc dataset to 6-option format...
01:02:18 - INFO - Processed 10000 instances for rc
01:02:18 - INFO - Splitting rc dataset (calibration: 50%, test: 50%)
01:02:18 - INFO - Split complete:
01:02:18 - INFO -   Calibration: 4999 instances
01:02:18 - INFO -   Test: 5001 instances
01:02:18 - INFO - Answer distribution:
01:02:18 - INFO -   Calibration: {'A': 1240, 'B': 1245, 'C': 1263, 'D': 1251}
01:02:18 - INFO -   Test: {'A': 1241, 'B': 1245, 'C': 1264, 'D': 1251}
01:02:18 - INFO - [dataset_processing] Duration: 36.28ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
01:02:18 - INFO - Initialized DemonstrationSelector
01:02:18 - INFO -   Strategy: random
01:02:18 - INFO -   Num demonstrations: 5
01:02:18 - INFO - Initialized DemonstrationManager
01:02:18 - INFO - Selecting 5 demonstrations using 'random' strategy
01:02:18 - INFO - Selected and cached 5 demonstrations for rc
01:02:18 - INFO - Loading model: gemma-2b-it (float16)
01:02:18 - INFO - Loading model: gemma-2b-it_float16 (google/gemma-2b-it)
01:02:18 - INFO - Loading tokenizer from google/gemma-2b-it
01:02:19 - INFO - Tokenizer loaded successfully
01:02:19 - INFO -   Vocab size: 256000
01:02:19 - INFO -   Padding side: left
01:02:19 - INFO -   PAD token: <pad> (ID: 0)
01:02:19 - INFO - Loading model from google/gemma-2b-it
01:02:19 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.56it/s]
01:02:21 - INFO - Model loaded successfully
01:02:21 - INFO -   GPU Memory: 4.68GB allocated, 39.35GB reserved

Model: gemma-2b-it_float16
  ID: google/gemma-2b-it
  Type: causal
  Parameters: 2,506,172,416
  Vocab size: 256,000
  Max length: 8192
  Device: cuda:0
  Dtype: torch.float16
  Instruct-tuned: True
01:02:21 - INFO - [model_loading] Duration: 2605.83ms | GPU Memory: 8.1MB -> 4788.3MB (delta +4780.1MB)
01:02:21 - INFO - [after_model_load_gemma-2b-it] GPU State: Allocated: 4788.3MB | Reserved: 40290.0MB | Free: 76367.5MB | Utilization: 0.0%
01:02:21 - INFO - Using batch size: 64
01:02:21 - INFO -   Strategy: base
01:02:21 - INFO - Initialized PromptBuilder for task: rc
01:02:21 - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
01:02:21 - INFO - Building 10000 prompts using 'base' strategy with 5 demonstrations
01:02:21 - INFO - Initialized InferenceEngine for gemma-2b-it_float16
01:02:21 - INFO -   Device: cuda:0
01:02:21 - INFO -   Batch size: 64
01:02:21 - INFO -   Option tokens: {'A': 586, 'B': 599, 'C': 585, 'D': 608, 'E': 637, 'F': 633}
Running inference:   0%|          | 0/79 [00:00<?, ?it/s]Running inference:   1%|▏         | 1/79 [00:02<02:38,  2.03s/it]Running inference:   3%|▎         | 2/79 [00:04<02:37,  2.05s/it]Running inference:   4%|▍         | 3/79 [00:06<02:36,  2.06s/it]Running inference:   5%|▌         | 4/79 [00:08<02:34,  2.06s/it]Running inference:   6%|▋         | 5/79 [00:10<02:32,  2.07s/it]Running inference:   8%|▊         | 6/79 [00:12<02:32,  2.09s/it]Running inference:   9%|▉         | 7/79 [00:14<02:30,  2.09s/it]Running inference:  10%|█         | 8/79 [00:16<02:27,  2.08s/it]Running inference:  11%|█▏        | 9/79 [00:18<02:27,  2.11s/it]Running inference:  13%|█▎        | 10/79 [00:20<02:25,  2.11s/it]Running inference:  14%|█▍        | 11/79 [00:22<02:22,  2.10s/it]Running inference:  15%|█▌        | 12/79 [00:25<02:20,  2.10s/it]Running inference:  16%|█▋        | 13/79 [00:27<02:19,  2.11s/it]Running inference:  18%|█▊        | 14/79 [00:29<02:17,  2.11s/it]Running inference:  19%|█▉        | 15/79 [00:31<02:14,  2.10s/it]Running inference:  20%|██        | 16/79 [00:33<02:12,  2.10s/it]Running inference:  22%|██▏       | 17/79 [00:35<02:10,  2.11s/it]Running inference:  23%|██▎       | 18/79 [00:37<02:08,  2.11s/it]Running inference:  24%|██▍       | 19/79 [00:39<02:06,  2.11s/it]Running inference:  25%|██▌       | 20/79 [00:41<02:04,  2.11s/it]Running inference:  27%|██▋       | 21/79 [00:44<02:02,  2.10s/it]Running inference:  28%|██▊       | 22/79 [00:46<01:59,  2.10s/it]Running inference:  29%|██▉       | 23/79 [00:48<01:56,  2.09s/it]Running inference:  30%|███       | 24/79 [00:50<01:54,  2.09s/it]Running inference:  32%|███▏      | 25/79 [00:52<01:53,  2.11s/it]Running inference:  33%|███▎      | 26/79 [00:54<01:51,  2.11s/it]Running inference:  34%|███▍      | 27/79 [00:56<01:49,  2.10s/it]Running inference:  35%|███▌      | 28/79 [00:58<01:47,  2.10s/it]Running inference:  37%|███▋      | 29/79 [01:00<01:44,  2.10s/it]Running inference:  38%|███▊      | 30/79 [01:02<01:42,  2.10s/it]Running inference:  39%|███▉      | 31/79 [01:05<01:41,  2.11s/it]Running inference:  41%|████      | 32/79 [01:07<01:39,  2.13s/it]Running inference:  42%|████▏     | 33/79 [01:09<01:36,  2.11s/it]Running inference:  43%|████▎     | 34/79 [01:11<01:34,  2.11s/it]Running inference:  44%|████▍     | 35/79 [01:13<01:33,  2.13s/it]Running inference:  46%|████▌     | 36/79 [01:15<01:30,  2.12s/it]Running inference:  47%|████▋     | 37/79 [01:17<01:29,  2.12s/it]Running inference:  48%|████▊     | 38/79 [01:19<01:27,  2.13s/it]Running inference:  49%|████▉     | 39/79 [01:22<01:24,  2.12s/it]Running inference:  51%|█████     | 40/79 [01:24<01:22,  2.11s/it]Running inference:  52%|█████▏    | 41/79 [01:26<01:19,  2.09s/it]Running inference:  53%|█████▎    | 42/79 [01:28<01:17,  2.11s/it]Running inference:  54%|█████▍    | 43/79 [01:30<01:15,  2.11s/it]Running inference:  56%|█████▌    | 44/79 [01:32<01:13,  2.10s/it]Running inference:  57%|█████▋    | 45/79 [01:34<01:11,  2.10s/it]Running inference:  58%|█████▊    | 46/79 [01:36<01:09,  2.11s/it]Running inference:  59%|█████▉    | 47/79 [01:38<01:07,  2.12s/it]Running inference:  61%|██████    | 48/79 [01:40<01:05,  2.10s/it]Running inference:  62%|██████▏   | 49/79 [01:43<01:03,  2.11s/it]Running inference:  63%|██████▎   | 50/79 [01:45<01:01,  2.11s/it]Running inference:  65%|██████▍   | 51/79 [01:47<00:59,  2.11s/it]Running inference:  66%|██████▌   | 52/79 [01:49<00:56,  2.10s/it]Running inference:  67%|██████▋   | 53/79 [01:52<01:02,  2.42s/it]Running inference:  68%|██████▊   | 54/79 [01:54<00:58,  2.33s/it]Running inference:  70%|██████▉   | 55/79 [01:57<01:00,  2.52s/it]Running inference:  71%|███████   | 56/79 [01:59<00:55,  2.39s/it]Running inference:  72%|███████▏  | 57/79 [02:01<00:50,  2.31s/it]Running inference:  73%|███████▎  | 58/79 [02:03<00:47,  2.24s/it]Running inference:  75%|███████▍  | 59/79 [02:05<00:43,  2.18s/it]Running inference:  76%|███████▌  | 60/79 [02:08<00:46,  2.42s/it]Running inference:  77%|███████▋  | 61/79 [02:11<00:41,  2.33s/it]Running inference:  78%|███████▊  | 62/79 [02:13<00:38,  2.25s/it]Running inference:  80%|███████▉  | 63/79 [02:15<00:35,  2.21s/it]Running inference:  81%|████████  | 64/79 [02:17<00:32,  2.18s/it]Running inference:  82%|████████▏ | 65/79 [02:19<00:30,  2.15s/it]Running inference:  84%|████████▎ | 66/79 [02:21<00:27,  2.13s/it]Running inference:  85%|████████▍ | 67/79 [02:23<00:25,  2.12s/it]Running inference:  86%|████████▌ | 68/79 [02:25<00:23,  2.12s/it]Running inference:  87%|████████▋ | 69/79 [02:27<00:21,  2.11s/it]Running inference:  89%|████████▊ | 70/79 [02:29<00:19,  2.11s/it]Running inference:  90%|████████▉ | 71/79 [02:32<00:19,  2.38s/it]Running inference:  91%|█████████ | 72/79 [02:35<00:16,  2.30s/it]Running inference:  92%|█████████▏| 73/79 [02:38<00:15,  2.54s/it]Running inference:  94%|█████████▎| 74/79 [02:40<00:12,  2.40s/it]Running inference:  95%|█████████▍| 75/79 [02:42<00:09,  2.31s/it]Running inference:  96%|█████████▌| 76/79 [02:44<00:06,  2.26s/it]Running inference:  97%|█████████▋| 77/79 [02:46<00:04,  2.21s/it]Running inference:  99%|█████████▊| 78/79 [02:48<00:02,  2.17s/it]Running inference: 100%|██████████| 79/79 [02:48<00:00,  1.59s/it]Running inference: 100%|██████████| 79/79 [02:48<00:00,  2.14s/it]
01:05:10 - INFO - [inference_calibration] Duration: 168867.27ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
Running inference:   0%|          | 0/79 [00:00<?, ?it/s]Running inference:   1%|▏         | 1/79 [00:02<02:42,  2.09s/it]Running inference:   3%|▎         | 2/79 [00:04<02:42,  2.11s/it]Running inference:   4%|▍         | 3/79 [00:06<02:39,  2.09s/it]Running inference:   5%|▌         | 4/79 [00:08<02:38,  2.11s/it]Running inference:   6%|▋         | 5/79 [00:10<02:35,  2.10s/it]Running inference:   8%|▊         | 6/79 [00:12<02:32,  2.09s/it]Running inference:   9%|▉         | 7/79 [00:14<02:29,  2.08s/it]Running inference:  10%|█         | 8/79 [00:16<02:27,  2.08s/it]Running inference:  11%|█▏        | 9/79 [00:18<02:25,  2.07s/it]Running inference:  13%|█▎        | 10/79 [00:20<02:24,  2.09s/it]Running inference:  14%|█▍        | 11/79 [00:22<02:22,  2.09s/it]Running inference:  15%|█▌        | 12/79 [00:25<02:21,  2.11s/it]Running inference:  16%|█▋        | 13/79 [00:28<02:39,  2.41s/it]Running inference:  18%|█▊        | 14/79 [00:30<02:30,  2.32s/it]Running inference:  19%|█▉        | 15/79 [00:32<02:26,  2.29s/it]Running inference:  20%|██        | 16/79 [00:34<02:21,  2.24s/it]Running inference:  22%|██▏       | 17/79 [00:37<02:35,  2.51s/it]Running inference:  23%|██▎       | 18/79 [00:39<02:25,  2.39s/it]Running inference:  24%|██▍       | 19/79 [00:42<02:18,  2.31s/it]Running inference:  25%|██▌       | 20/79 [00:44<02:12,  2.25s/it]Running inference:  27%|██▋       | 21/79 [00:46<02:08,  2.22s/it]Running inference:  28%|██▊       | 22/79 [00:48<02:05,  2.19s/it]Running inference:  29%|██▉       | 23/79 [00:50<02:01,  2.18s/it]Running inference:  30%|███       | 24/79 [00:52<01:58,  2.15s/it]Running inference:  32%|███▏      | 25/79 [00:54<01:55,  2.14s/it]Running inference:  33%|███▎      | 26/79 [00:57<01:54,  2.16s/it]Running inference:  34%|███▍      | 27/79 [00:59<01:51,  2.15s/it]Running inference:  35%|███▌      | 28/79 [01:01<01:49,  2.14s/it]Running inference:  37%|███▋      | 29/79 [01:03<01:45,  2.11s/it]Running inference:  38%|███▊      | 30/79 [01:05<01:43,  2.11s/it]Running inference:  39%|███▉      | 31/79 [01:07<01:41,  2.12s/it]Running inference:  41%|████      | 32/79 [01:09<01:39,  2.13s/it]Running inference:  42%|████▏     | 33/79 [01:11<01:37,  2.12s/it]Running inference:  43%|████▎     | 34/79 [01:13<01:34,  2.09s/it]Running inference:  44%|████▍     | 35/79 [01:15<01:32,  2.10s/it]Running inference:  46%|████▌     | 36/79 [01:18<01:30,  2.11s/it]Running inference:  47%|████▋     | 37/79 [01:20<01:29,  2.12s/it]Running inference:  48%|████▊     | 38/79 [01:22<01:26,  2.10s/it]Running inference:  49%|████▉     | 39/79 [01:24<01:24,  2.10s/it]Running inference:  51%|█████     | 40/79 [01:26<01:22,  2.12s/it]Running inference:  52%|█████▏    | 41/79 [01:28<01:20,  2.12s/it]Running inference:  53%|█████▎    | 42/79 [01:30<01:18,  2.13s/it]Running inference:  54%|█████▍    | 43/79 [01:32<01:16,  2.12s/it]Running inference:  56%|█████▌    | 44/79 [01:35<01:14,  2.14s/it]Running inference:  57%|█████▋    | 45/79 [01:37<01:12,  2.14s/it]Running inference:  58%|█████▊    | 46/79 [01:39<01:10,  2.13s/it]Running inference:  59%|█████▉    | 47/79 [01:41<01:07,  2.12s/it]Running inference:  61%|██████    | 48/79 [01:43<01:05,  2.12s/it]Running inference:  62%|██████▏   | 49/79 [01:45<01:02,  2.10s/it]Running inference:  63%|██████▎   | 50/79 [01:47<01:01,  2.11s/it]Running inference:  65%|██████▍   | 51/79 [01:49<00:59,  2.11s/it]Running inference:  66%|██████▌   | 52/79 [01:51<00:56,  2.11s/it]Running inference:  67%|██████▋   | 53/79 [01:56<01:10,  2.70s/it]Running inference:  68%|██████▊   | 54/79 [01:58<01:03,  2.54s/it]Running inference:  70%|██████▉   | 55/79 [02:00<00:57,  2.41s/it]Running inference:  71%|███████   | 56/79 [02:02<00:52,  2.30s/it]Running inference:  72%|███████▏  | 57/79 [02:04<00:49,  2.26s/it]Running inference:  73%|███████▎  | 58/79 [02:06<00:46,  2.21s/it]Running inference:  75%|███████▍  | 59/79 [02:08<00:43,  2.18s/it]Running inference:  76%|███████▌  | 60/79 [02:10<00:41,  2.19s/it]Running inference:  77%|███████▋  | 61/79 [02:14<00:44,  2.48s/it]Running inference:  78%|███████▊  | 62/79 [02:16<00:40,  2.36s/it]Running inference:  80%|███████▉  | 63/79 [02:18<00:36,  2.28s/it]Running inference:  81%|████████  | 64/79 [02:20<00:33,  2.23s/it]Running inference:  82%|████████▏ | 65/79 [02:22<00:30,  2.19s/it]Running inference:  84%|████████▎ | 66/79 [02:24<00:28,  2.17s/it]Running inference:  85%|████████▍ | 67/79 [02:26<00:25,  2.15s/it]Running inference:  86%|████████▌ | 68/79 [02:28<00:23,  2.13s/it]Running inference:  87%|████████▋ | 69/79 [02:30<00:21,  2.13s/it]Running inference:  89%|████████▊ | 70/79 [02:33<00:19,  2.14s/it]Running inference:  90%|████████▉ | 71/79 [02:35<00:16,  2.12s/it]Running inference:  91%|█████████ | 72/79 [02:37<00:14,  2.11s/it]Running inference:  92%|█████████▏| 73/79 [02:39<00:12,  2.15s/it]Running inference:  94%|█████████▎| 74/79 [02:41<00:10,  2.13s/it]Running inference:  95%|█████████▍| 75/79 [02:43<00:08,  2.13s/it]Running inference:  96%|█████████▌| 76/79 [02:45<00:06,  2.13s/it]Running inference:  97%|█████████▋| 77/79 [02:47<00:04,  2.12s/it]Running inference:  99%|█████████▊| 78/79 [02:50<00:02,  2.11s/it]Running inference: 100%|██████████| 79/79 [02:50<00:00,  1.56s/it]Running inference: 100%|██████████| 79/79 [02:50<00:00,  2.16s/it]
01:08:00 - INFO - [inference_test] Duration: 170285.22ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
01:08:00 - INFO -     Inference: 339.15s for 10000 samples (29.49 samples/sec)
01:08:00 - INFO - Initialized ProbabilityExtractor
01:08:00 - INFO -   Temperature: 1.0
01:08:00 - INFO -   Calibration method: None
01:08:00 - INFO - [probability_extraction] Duration: 273.99ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
01:08:00 - INFO -     Raw probabilities saved to: outputs/results/probabilities/probs_gemma-2b-it_rc_float16_base_20251208_010140.npz
01:08:00 - INFO - Initialized LACScorer
01:08:00 - INFO -   Alpha: 0.1
01:08:00 - INFO -   Target coverage: 90.0%
01:08:00 - INFO - Initialized LAC (Least Ambiguous set-valued Classifiers) scorer
01:08:00 - INFO - Initialized PredictionSetGenerator
01:08:00 - INFO -   Methods: ['lac']
01:08:00 - INFO -   Alpha: 0.1
01:08:00 - INFO -   Aggregation: separate
01:08:00 - INFO - Calibrating 1 conformal predictors...
01:08:00 - INFO -   Calibrating LAC...
01:08:00 - INFO - Calibrating with 4999 samples...
01:08:00 - INFO - Calibration complete
01:08:00 - INFO -   Threshold: 0.9958
01:08:00 - INFO -   Score range: [0.0000, 1.0000]
01:08:00 - INFO - Calibration complete
01:08:00 - INFO - Generating prediction sets using LAC...
01:08:00 - INFO - Generating prediction sets for 5001 test instances...
01:08:00 - INFO - Prediction complete
01:08:00 - INFO -   Average set size: 5.27
01:08:00 - INFO -   Coverage rate: 90.40%
01:08:00 - INFO -   Meets coverage guarantee: True
01:08:00 - INFO - [PASS] Coverage guarantee met: 90.40% >= 90.00%
01:08:00 - INFO -     LAC: Acc=22.86%, CR=90.40%, SS=5.27
01:08:00 - INFO - Initialized APSScorer
01:08:00 - INFO -   Alpha: 0.1
01:08:00 - INFO -   Target coverage: 90.0%
01:08:00 - INFO - Initialized APS (Adaptive Prediction Sets) scorer
01:08:00 - INFO - Initialized PredictionSetGenerator
01:08:00 - INFO -   Methods: ['aps']
01:08:00 - INFO -   Alpha: 0.1
01:08:00 - INFO -   Aggregation: separate
01:08:00 - INFO - Calibrating 1 conformal predictors...
01:08:00 - INFO -   Calibrating APS...
01:08:00 - INFO - Calibrating with 4999 samples...
01:08:00 - INFO - Calibration complete
01:08:00 - INFO -   Threshold: 1.0000
01:08:00 - INFO -   Score range: [0.2081, 1.0000]
01:08:00 - INFO - Calibration complete
01:08:00 - INFO - Generating prediction sets using APS...
01:08:00 - INFO - Generating prediction sets for 5001 test instances...
01:08:01 - INFO - Prediction complete
01:08:01 - INFO -   Average set size: 5.17
01:08:01 - INFO -   Coverage rate: 90.66%
01:08:01 - INFO -   Meets coverage guarantee: True
01:08:01 - INFO - [PASS] Coverage guarantee met: 90.66% >= 90.00%
01:08:01 - INFO -     APS: Acc=22.86%, CR=90.66%, SS=5.17
01:08:05 - INFO - Unloaded model: gemma-2b-it_float16
01:08:06 - INFO - Checkpoint saved after completing: gemma-2b-it | rc | float16
01:08:06 - INFO - 
================================================================================
01:08:06 - INFO - Run 3/5: gemma-2b-it | ci | float16
01:08:06 - INFO - ================================================================================
01:08:06 - INFO - [start_gemma-2b-it_ci] GPU State: Allocated: 8.1MB | Reserved: 4780.0MB | Free: 81147.6MB | Utilization: 100.0%
01:08:06 - INFO - Loading ci dataset (10000 samples)...
01:08:06 - INFO - Loading HellaSwag dataset with 10000 samples...
Downloading readme: 0.00B [00:00, ?B/s]Downloading readme: 7.02kB [00:00, 24.7MB/s]
01:08:10 - INFO - Loaded 10000 HellaSwag instances
01:08:10 - INFO - [dataset_loading] Duration: 4010.28ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
01:08:10 - INFO - Processing ci dataset to 6-option format...
01:08:10 - INFO - Processed 10000 instances for ci
01:08:10 - INFO - Splitting ci dataset (calibration: 50%, test: 50%)
01:08:10 - INFO - Split complete:
01:08:10 - INFO -   Calibration: 4999 instances
01:08:10 - INFO -   Test: 5001 instances
01:08:10 - INFO - Answer distribution:
01:08:10 - INFO -   Calibration: {'A': 1240, 'B': 1242, 'C': 1256, 'D': 1261}
01:08:10 - INFO -   Test: {'A': 1240, 'B': 1242, 'C': 1257, 'D': 1262}
01:08:10 - INFO - [dataset_processing] Duration: 41.93ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
01:08:10 - INFO - Initialized DemonstrationSelector
01:08:10 - INFO -   Strategy: random
01:08:10 - INFO -   Num demonstrations: 5
01:08:10 - INFO - Initialized DemonstrationManager
01:08:10 - INFO - Selecting 5 demonstrations using 'random' strategy
01:08:10 - INFO - Selected and cached 5 demonstrations for ci
01:08:10 - INFO - Loading model: gemma-2b-it (float16)
01:08:10 - INFO - Loading model: gemma-2b-it_float16 (google/gemma-2b-it)
01:08:10 - INFO - Loading tokenizer from google/gemma-2b-it
01:08:11 - INFO - Tokenizer loaded successfully
01:08:11 - INFO -   Vocab size: 256000
01:08:11 - INFO -   Padding side: left
01:08:11 - INFO -   PAD token: <pad> (ID: 0)
01:08:11 - INFO - Loading model from google/gemma-2b-it
01:08:11 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.55it/s]
01:08:12 - INFO - Model loaded successfully
01:08:12 - INFO -   GPU Memory: 4.68GB allocated, 4.77GB reserved

Model: gemma-2b-it_float16
  ID: google/gemma-2b-it
  Type: causal
  Parameters: 2,506,172,416
  Vocab size: 256,000
  Max length: 8192
  Device: cuda:0
  Dtype: torch.float16
  Instruct-tuned: True
01:08:12 - INFO - [model_loading] Duration: 2525.51ms | GPU Memory: 8.1MB -> 4788.3MB (delta +4780.1MB)
01:08:12 - INFO - [after_model_load_gemma-2b-it] GPU State: Allocated: 4788.3MB | Reserved: 4882.0MB | Free: 76367.5MB | Utilization: 1.0%
01:08:12 - INFO - Using batch size: 64
01:08:12 - INFO -   Strategy: base
01:08:12 - INFO - Initialized PromptBuilder for task: ci
01:08:12 - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
01:08:12 - INFO - Building 10000 prompts using 'base' strategy with 5 demonstrations
01:08:12 - INFO - Initialized InferenceEngine for gemma-2b-it_float16
01:08:12 - INFO -   Device: cuda:0
01:08:12 - INFO -   Batch size: 64
01:08:12 - INFO -   Option tokens: {'A': 586, 'B': 599, 'C': 585, 'D': 608, 'E': 637, 'F': 633}
Running inference:   0%|          | 0/79 [00:00<?, ?it/s]Running inference:   1%|▏         | 1/79 [00:02<02:45,  2.12s/it]Running inference:   3%|▎         | 2/79 [00:04<02:42,  2.11s/it]Running inference:   4%|▍         | 3/79 [00:06<02:39,  2.10s/it]Running inference:   5%|▌         | 4/79 [00:08<02:37,  2.10s/it]Running inference:   6%|▋         | 5/79 [00:10<02:35,  2.10s/it]Running inference:   8%|▊         | 6/79 [00:12<02:33,  2.10s/it]Running inference:   9%|▉         | 7/79 [00:14<02:31,  2.11s/it]Running inference:  10%|█         | 8/79 [00:17<02:54,  2.46s/it]Running inference:  11%|█▏        | 9/79 [00:20<02:44,  2.35s/it]Running inference:  13%|█▎        | 10/79 [00:22<02:37,  2.28s/it]Running inference:  14%|█▍        | 11/79 [00:24<02:30,  2.22s/it]Running inference:  15%|█▌        | 12/79 [00:26<02:25,  2.18s/it]Running inference:  16%|█▋        | 13/79 [00:28<02:21,  2.15s/it]Running inference:  18%|█▊        | 14/79 [00:30<02:18,  2.13s/it]Running inference:  19%|█▉        | 15/79 [00:32<02:15,  2.12s/it]Running inference:  20%|██        | 16/79 [00:34<02:13,  2.11s/it]Running inference:  22%|██▏       | 17/79 [00:36<02:10,  2.10s/it]Running inference:  23%|██▎       | 18/79 [00:38<02:08,  2.11s/it]Running inference:  24%|██▍       | 19/79 [00:41<02:06,  2.11s/it]Running inference:  25%|██▌       | 20/79 [00:43<02:04,  2.10s/it]Running inference:  27%|██▋       | 21/79 [00:45<02:01,  2.10s/it]Running inference:  28%|██▊       | 22/79 [00:47<01:59,  2.10s/it]Running inference:  29%|██▉       | 23/79 [00:49<01:57,  2.10s/it]Running inference:  30%|███       | 24/79 [00:51<01:55,  2.11s/it]Running inference:  32%|███▏      | 25/79 [00:54<02:08,  2.39s/it]Running inference:  33%|███▎      | 26/79 [00:57<02:19,  2.63s/it]Running inference:  34%|███▍      | 27/79 [00:59<02:08,  2.48s/it]Running inference:  35%|███▌      | 28/79 [01:01<02:00,  2.37s/it]Running inference:  37%|███▋      | 29/79 [01:04<01:54,  2.29s/it]Running inference:  38%|███▊      | 30/79 [01:06<01:49,  2.23s/it]Running inference:  39%|███▉      | 31/79 [01:08<01:45,  2.20s/it]Running inference:  41%|████      | 32/79 [01:10<01:42,  2.18s/it]Running inference:  42%|████▏     | 33/79 [01:12<01:39,  2.16s/it]Running inference:  43%|████▎     | 34/79 [01:14<01:36,  2.14s/it]Running inference:  44%|████▍     | 35/79 [01:16<01:34,  2.14s/it]Running inference:  46%|████▌     | 36/79 [01:18<01:31,  2.13s/it]Running inference:  47%|████▋     | 37/79 [01:20<01:28,  2.11s/it]Running inference:  48%|████▊     | 38/79 [01:23<01:26,  2.11s/it]Running inference:  49%|████▉     | 39/79 [01:25<01:24,  2.11s/it]Running inference:  51%|█████     | 40/79 [01:27<01:22,  2.11s/it]Running inference:  52%|█████▏    | 41/79 [01:29<01:20,  2.11s/it]Running inference:  53%|█████▎    | 42/79 [01:31<01:17,  2.10s/it]Running inference:  54%|█████▍    | 43/79 [01:33<01:15,  2.11s/it]Running inference:  56%|█████▌    | 44/79 [01:35<01:13,  2.11s/it]Running inference:  57%|█████▋    | 45/79 [01:37<01:11,  2.11s/it]Running inference:  58%|█████▊    | 46/79 [01:39<01:09,  2.11s/it]Running inference:  59%|█████▉    | 47/79 [01:42<01:07,  2.11s/it]Running inference:  61%|██████    | 48/79 [01:44<01:05,  2.12s/it]Running inference:  62%|██████▏   | 49/79 [01:46<01:03,  2.12s/it]Running inference:  63%|██████▎   | 50/79 [01:48<01:02,  2.16s/it]Running inference:  65%|██████▍   | 51/79 [01:50<01:00,  2.14s/it]Running inference:  66%|██████▌   | 52/79 [01:52<00:58,  2.17s/it]Running inference:  67%|██████▋   | 53/79 [01:55<00:55,  2.15s/it]Running inference:  68%|██████▊   | 54/79 [01:57<00:53,  2.13s/it]Running inference:  70%|██████▉   | 55/79 [01:59<00:50,  2.12s/it]Running inference:  71%|███████   | 56/79 [02:01<00:48,  2.12s/it]Running inference:  72%|███████▏  | 57/79 [02:03<00:46,  2.11s/it]Running inference:  73%|███████▎  | 58/79 [02:05<00:44,  2.10s/it]Running inference:  75%|███████▍  | 59/79 [02:07<00:42,  2.12s/it]Running inference:  76%|███████▌  | 60/79 [02:09<00:40,  2.13s/it]Running inference:  77%|███████▋  | 61/79 [02:11<00:38,  2.12s/it]Running inference:  78%|███████▊  | 62/79 [02:13<00:35,  2.11s/it]Running inference:  80%|███████▉  | 63/79 [02:16<00:33,  2.11s/it]Running inference:  81%|████████  | 64/79 [02:18<00:31,  2.11s/it]Running inference:  82%|████████▏ | 65/79 [02:20<00:29,  2.10s/it]Running inference:  84%|████████▎ | 66/79 [02:22<00:27,  2.10s/it]Running inference:  85%|████████▍ | 67/79 [02:24<00:25,  2.10s/it]Running inference:  86%|████████▌ | 68/79 [02:26<00:23,  2.12s/it]Running inference:  87%|████████▋ | 69/79 [02:28<00:21,  2.11s/it]Running inference:  89%|████████▊ | 70/79 [02:30<00:18,  2.11s/it]Running inference:  90%|████████▉ | 71/79 [02:32<00:16,  2.11s/it]Running inference:  91%|█████████ | 72/79 [02:35<00:14,  2.12s/it]Running inference:  92%|█████████▏| 73/79 [02:37<00:12,  2.12s/it]Running inference:  94%|█████████▎| 74/79 [02:39<00:10,  2.12s/it]Running inference:  95%|█████████▍| 75/79 [02:41<00:08,  2.12s/it]Running inference:  96%|█████████▌| 76/79 [02:43<00:06,  2.12s/it]Running inference:  97%|█████████▋| 77/79 [02:45<00:04,  2.11s/it]Running inference:  99%|█████████▊| 78/79 [02:47<00:02,  2.11s/it]Running inference: 100%|██████████| 79/79 [02:47<00:00,  1.55s/it]Running inference: 100%|██████████| 79/79 [02:47<00:00,  2.13s/it]
01:11:00 - INFO - [inference_calibration] Duration: 167995.01ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
Running inference:   0%|          | 0/79 [00:00<?, ?it/s]Running inference:   1%|▏         | 1/79 [00:02<02:46,  2.13s/it]Running inference:   3%|▎         | 2/79 [00:04<02:42,  2.11s/it]Running inference:   4%|▍         | 3/79 [00:06<02:41,  2.13s/it]Running inference:   5%|▌         | 4/79 [00:08<02:39,  2.12s/it]Running inference:   6%|▋         | 5/79 [00:10<02:37,  2.13s/it]Running inference:   8%|▊         | 6/79 [00:12<02:35,  2.13s/it]Running inference:   9%|▉         | 7/79 [00:14<02:32,  2.11s/it]Running inference:  10%|█         | 8/79 [00:16<02:29,  2.11s/it]Running inference:  11%|█▏        | 9/79 [00:19<02:27,  2.10s/it]Running inference:  13%|█▎        | 10/79 [00:21<02:25,  2.11s/it]Running inference:  14%|█▍        | 11/79 [00:23<02:23,  2.11s/it]Running inference:  15%|█▌        | 12/79 [00:25<02:22,  2.12s/it]Running inference:  16%|█▋        | 13/79 [00:27<02:21,  2.14s/it]Running inference:  18%|█▊        | 14/79 [00:29<02:18,  2.13s/it]Running inference:  19%|█▉        | 15/79 [00:31<02:16,  2.14s/it]Running inference:  20%|██        | 16/79 [00:33<02:13,  2.13s/it]Running inference:  22%|██▏       | 17/79 [00:36<02:11,  2.12s/it]Running inference:  23%|██▎       | 18/79 [00:38<02:08,  2.11s/it]Running inference:  24%|██▍       | 19/79 [00:40<02:06,  2.11s/it]Running inference:  25%|██▌       | 20/79 [00:42<02:04,  2.10s/it]Running inference:  27%|██▋       | 21/79 [00:44<02:01,  2.10s/it]Running inference:  28%|██▊       | 22/79 [00:46<01:59,  2.09s/it]Running inference:  29%|██▉       | 23/79 [00:48<01:57,  2.10s/it]Running inference:  30%|███       | 24/79 [00:50<01:55,  2.10s/it]Running inference:  32%|███▏      | 25/79 [00:52<01:54,  2.11s/it]Running inference:  33%|███▎      | 26/79 [00:54<01:51,  2.11s/it]Running inference:  34%|███▍      | 27/79 [00:57<01:50,  2.12s/it]Running inference:  35%|███▌      | 28/79 [00:59<01:47,  2.11s/it]Running inference:  37%|███▋      | 29/79 [01:01<01:45,  2.12s/it]Running inference:  38%|███▊      | 30/79 [01:03<01:43,  2.11s/it]Running inference:  39%|███▉      | 31/79 [01:05<01:41,  2.11s/it]Running inference:  41%|████      | 32/79 [01:07<01:39,  2.11s/it]Running inference:  42%|████▏     | 33/79 [01:09<01:36,  2.11s/it]Running inference:  43%|████▎     | 34/79 [01:11<01:35,  2.12s/it]Running inference:  44%|████▍     | 35/79 [01:14<01:33,  2.13s/it]Running inference:  46%|████▌     | 36/79 [01:16<01:32,  2.14s/it]Running inference:  47%|████▋     | 37/79 [01:18<01:29,  2.13s/it]Running inference:  48%|████▊     | 38/79 [01:20<01:26,  2.12s/it]Running inference:  49%|████▉     | 39/79 [01:22<01:24,  2.11s/it]Running inference:  51%|█████     | 40/79 [01:24<01:22,  2.11s/it]Running inference:  52%|█████▏    | 41/79 [01:26<01:20,  2.12s/it]Running inference:  53%|█████▎    | 42/79 [01:28<01:18,  2.12s/it]Running inference:  54%|█████▍    | 43/79 [01:30<01:16,  2.12s/it]Running inference:  56%|█████▌    | 44/79 [01:33<01:13,  2.11s/it]Running inference:  57%|█████▋    | 45/79 [01:35<01:11,  2.11s/it]Running inference:  58%|█████▊    | 46/79 [01:37<01:09,  2.11s/it]Running inference:  59%|█████▉    | 47/79 [01:39<01:07,  2.12s/it]Running inference:  61%|██████    | 48/79 [01:41<01:05,  2.11s/it]Running inference:  62%|██████▏   | 49/79 [01:43<01:03,  2.12s/it]Running inference:  63%|██████▎   | 50/79 [01:45<01:01,  2.12s/it]Running inference:  65%|██████▍   | 51/79 [01:48<01:07,  2.40s/it]Running inference:  66%|██████▌   | 52/79 [01:50<01:02,  2.32s/it]Running inference:  67%|██████▋   | 53/79 [01:53<00:58,  2.25s/it]Running inference:  68%|██████▊   | 54/79 [01:55<00:54,  2.19s/it]Running inference:  70%|██████▉   | 55/79 [01:58<00:59,  2.50s/it]Running inference:  71%|███████   | 56/79 [02:00<00:55,  2.40s/it]Running inference:  72%|███████▏  | 57/79 [02:02<00:50,  2.32s/it]Running inference:  73%|███████▎  | 58/79 [02:04<00:47,  2.24s/it]Running inference:  75%|███████▍  | 59/79 [02:06<00:44,  2.20s/it]Running inference:  76%|███████▌  | 60/79 [02:09<00:46,  2.46s/it]Running inference:  77%|███████▋  | 61/79 [02:11<00:42,  2.36s/it]Running inference:  78%|███████▊  | 62/79 [02:14<00:38,  2.28s/it]Running inference:  80%|███████▉  | 63/79 [02:16<00:35,  2.22s/it]Running inference:  81%|████████  | 64/79 [02:18<00:32,  2.19s/it]Running inference:  82%|████████▏ | 65/79 [02:20<00:30,  2.17s/it]Running inference:  84%|████████▎ | 66/79 [02:22<00:27,  2.15s/it]Running inference:  85%|████████▍ | 67/79 [02:25<00:29,  2.44s/it]Running inference:  86%|████████▌ | 68/79 [02:27<00:25,  2.35s/it]Running inference:  87%|████████▋ | 69/79 [02:29<00:22,  2.27s/it]Running inference:  89%|████████▊ | 70/79 [02:31<00:20,  2.22s/it]Running inference:  90%|████████▉ | 71/79 [02:34<00:17,  2.21s/it]Running inference:  91%|█████████ | 72/79 [02:36<00:15,  2.18s/it]Running inference:  92%|█████████▏| 73/79 [02:38<00:13,  2.17s/it]Running inference:  94%|█████████▎| 74/79 [02:40<00:10,  2.15s/it]Running inference:  95%|█████████▍| 75/79 [02:42<00:08,  2.14s/it]Running inference:  96%|█████████▌| 76/79 [02:44<00:06,  2.13s/it]Running inference:  97%|█████████▋| 77/79 [02:46<00:04,  2.14s/it]Running inference:  99%|█████████▊| 78/79 [02:49<00:02,  2.16s/it]Running inference: 100%|██████████| 79/79 [02:49<00:00,  1.60s/it]Running inference: 100%|██████████| 79/79 [02:49<00:00,  2.14s/it]
01:13:50 - INFO - [inference_test] Duration: 169352.01ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
01:13:50 - INFO -     Inference: 337.35s for 10000 samples (29.64 samples/sec)
01:13:50 - INFO - Initialized ProbabilityExtractor
01:13:50 - INFO -   Temperature: 1.0
01:13:50 - INFO -   Calibration method: None
01:13:50 - INFO - [probability_extraction] Duration: 292.58ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
01:13:50 - INFO -     Raw probabilities saved to: outputs/results/probabilities/probs_gemma-2b-it_ci_float16_base_20251208_010140.npz
01:13:50 - INFO - Initialized LACScorer
01:13:50 - INFO -   Alpha: 0.1
01:13:50 - INFO -   Target coverage: 90.0%
01:13:50 - INFO - Initialized LAC (Least Ambiguous set-valued Classifiers) scorer
01:13:50 - INFO - Initialized PredictionSetGenerator
01:13:50 - INFO -   Methods: ['lac']
01:13:50 - INFO -   Alpha: 0.1
01:13:50 - INFO -   Aggregation: separate
01:13:50 - INFO - Calibrating 1 conformal predictors...
01:13:50 - INFO -   Calibrating LAC...
01:13:50 - INFO - Calibrating with 4999 samples...
01:13:50 - INFO - Calibration complete
01:13:50 - INFO -   Threshold: 0.9965
01:13:50 - INFO -   Score range: [0.0000, 1.0000]
01:13:50 - INFO - Calibration complete
01:13:50 - INFO - Generating prediction sets using LAC...
01:13:50 - INFO - Generating prediction sets for 5001 test instances...
01:13:50 - INFO - Prediction complete
01:13:50 - INFO -   Average set size: 5.32
01:13:50 - INFO -   Coverage rate: 90.44%
01:13:50 - INFO -   Meets coverage guarantee: True
01:13:50 - INFO - [PASS] Coverage guarantee met: 90.44% >= 90.00%
01:13:50 - INFO -     LAC: Acc=22.08%, CR=90.44%, SS=5.32
01:13:50 - INFO - Initialized APSScorer
01:13:50 - INFO -   Alpha: 0.1
01:13:50 - INFO -   Target coverage: 90.0%
01:13:50 - INFO - Initialized APS (Adaptive Prediction Sets) scorer
01:13:50 - INFO - Initialized PredictionSetGenerator
01:13:50 - INFO -   Methods: ['aps']
01:13:50 - INFO -   Alpha: 0.1
01:13:50 - INFO -   Aggregation: separate
01:13:50 - INFO - Calibrating 1 conformal predictors...
01:13:50 - INFO -   Calibrating APS...
01:13:50 - INFO - Calibrating with 4999 samples...
01:13:50 - INFO - Calibration complete
01:13:50 - INFO -   Threshold: 1.0000
01:13:50 - INFO -   Score range: [0.2352, 1.0000]
01:13:50 - INFO - Calibration complete
01:13:50 - INFO - Generating prediction sets using APS...
01:13:50 - INFO - Generating prediction sets for 5001 test instances...
01:13:50 - INFO - Prediction complete
01:13:50 - INFO -   Average set size: 5.86
01:13:50 - INFO -   Coverage rate: 98.54%
01:13:50 - INFO -   Meets coverage guarantee: True
01:13:50 - INFO - [PASS] Coverage guarantee met: 98.54% >= 90.00%
01:13:50 - INFO -     APS: Acc=22.08%, CR=98.54%, SS=5.86
01:13:55 - INFO - Unloaded model: gemma-2b-it_float16
01:13:55 - INFO - Checkpoint saved after completing: gemma-2b-it | ci | float16
01:13:55 - INFO - 
================================================================================
01:13:55 - INFO - Run 4/5: gemma-2b-it | drs | float16
01:13:55 - INFO - ================================================================================
01:13:55 - INFO - [start_gemma-2b-it_drs] GPU State: Allocated: 8.1MB | Reserved: 4780.0MB | Free: 81147.6MB | Utilization: 100.0%
01:13:55 - INFO - Loading drs dataset (10000 samples)...
01:13:55 - INFO - Loading HaluDial dataset with 10000 samples...
Downloading readme: 0.00B [00:00, ?B/s]Downloading readme: 2.88kB [00:00, 16.5MB/s]
01:13:58 - INFO - Loaded 10000 HaluDial instances
01:13:58 - INFO - [dataset_loading] Duration: 2131.13ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
01:13:58 - INFO - Processing drs dataset to 6-option format...
01:14:31 - INFO - Processed 10000 instances for drs
01:14:31 - INFO - Option expansion statistics for drs:
01:14:31 - INFO -   Instances expanded (2→4 options): 10000
01:14:31 - INFO -   Total options sampled: 20000
01:14:31 - INFO -   Duplicate options avoided: 2
01:14:31 - INFO -   Fallback options used: 0
01:14:31 - INFO - Splitting drs dataset (calibration: 50%, test: 50%)
01:14:31 - INFO - Split complete:
01:14:31 - INFO -   Calibration: 4999 instances
01:14:31 - INFO -   Test: 5001 instances
01:14:31 - INFO - Answer distribution:
01:14:31 - INFO -   Calibration: {'A': 1258, 'B': 1203, 'C': 1264, 'D': 1274}
01:14:31 - INFO -   Test: {'A': 1258, 'B': 1204, 'C': 1265, 'D': 1274}
01:14:31 - INFO - [dataset_processing] Duration: 33612.38ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
01:14:31 - INFO - Initialized DemonstrationSelector
01:14:31 - INFO -   Strategy: random
01:14:31 - INFO -   Num demonstrations: 5
01:14:31 - INFO - Initialized DemonstrationManager
01:14:31 - INFO - Selecting 3 demonstrations using 'random' strategy
01:14:31 - INFO - Selected and cached 3 demonstrations for drs
01:14:31 - INFO - Loading model: gemma-2b-it (float16)
01:14:31 - INFO - Loading model: gemma-2b-it_float16 (google/gemma-2b-it)
01:14:31 - INFO - Loading tokenizer from google/gemma-2b-it
01:14:32 - INFO - Tokenizer loaded successfully
01:14:32 - INFO -   Vocab size: 256000
01:14:32 - INFO -   Padding side: left
01:14:32 - INFO -   PAD token: <pad> (ID: 0)
01:14:32 - INFO - Loading model from google/gemma-2b-it
01:14:32 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]
01:14:34 - INFO - Model loaded successfully
01:14:34 - INFO -   GPU Memory: 4.68GB allocated, 4.77GB reserved

Model: gemma-2b-it_float16
  ID: google/gemma-2b-it
  Type: causal
  Parameters: 2,506,172,416
  Vocab size: 256,000
  Max length: 8192
  Device: cuda:0
  Dtype: torch.float16
  Instruct-tuned: True
01:14:34 - INFO - [model_loading] Duration: 2533.22ms | GPU Memory: 8.1MB -> 4788.3MB (delta +4780.1MB)
01:14:34 - INFO - [after_model_load_gemma-2b-it] GPU State: Allocated: 4788.3MB | Reserved: 4882.0MB | Free: 76367.5MB | Utilization: 2.0%
01:14:34 - INFO - Using batch size: 64
01:14:34 - INFO -   Strategy: base
01:14:34 - INFO - Initialized PromptBuilder for task: drs
01:14:34 - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
01:14:34 - INFO - Building 10000 prompts using 'base' strategy with 3 demonstrations
01:14:34 - INFO - Initialized InferenceEngine for gemma-2b-it_float16
01:14:34 - INFO -   Device: cuda:0
01:14:34 - INFO -   Batch size: 64
01:14:34 - INFO -   Option tokens: {'A': 586, 'B': 599, 'C': 585, 'D': 608, 'E': 637, 'F': 633}
Running inference:   0%|          | 0/79 [00:00<?, ?it/s]Running inference:   1%|▏         | 1/79 [00:01<01:43,  1.33s/it]Running inference:   3%|▎         | 2/79 [00:02<01:41,  1.31s/it]Running inference:   4%|▍         | 3/79 [00:03<01:39,  1.31s/it]Running inference:   5%|▌         | 4/79 [00:05<01:39,  1.32s/it]Running inference:   6%|▋         | 5/79 [00:06<01:37,  1.32s/it]Running inference:   8%|▊         | 6/79 [00:07<01:35,  1.30s/it]Running inference:   9%|▉         | 7/79 [00:09<01:35,  1.33s/it]Running inference:  10%|█         | 8/79 [00:10<01:33,  1.31s/it]Running inference:  11%|█▏        | 9/79 [00:11<01:31,  1.30s/it]Running inference:  13%|█▎        | 10/79 [00:13<01:29,  1.30s/it]Running inference:  14%|█▍        | 11/79 [00:14<01:28,  1.30s/it]Running inference:  15%|█▌        | 12/79 [00:16<01:43,  1.54s/it]Running inference:  16%|█▋        | 13/79 [00:17<01:38,  1.49s/it]Running inference:  18%|█▊        | 14/79 [00:19<01:33,  1.43s/it]Running inference:  19%|█▉        | 15/79 [00:20<01:28,  1.39s/it]Running inference:  20%|██        | 16/79 [00:21<01:25,  1.35s/it]Running inference:  22%|██▏       | 17/79 [00:23<01:27,  1.41s/it]Running inference:  23%|██▎       | 18/79 [00:24<01:24,  1.39s/it]Running inference:  24%|██▍       | 19/79 [00:25<01:21,  1.36s/it]Running inference:  25%|██▌       | 20/79 [00:27<01:19,  1.35s/it]Running inference:  27%|██▋       | 21/79 [00:28<01:18,  1.36s/it]Running inference:  28%|██▊       | 22/79 [00:29<01:17,  1.35s/it]Running inference:  29%|██▉       | 23/79 [00:31<01:14,  1.34s/it]Running inference:  30%|███       | 24/79 [00:32<01:13,  1.34s/it]Running inference:  32%|███▏      | 25/79 [00:33<01:12,  1.35s/it]Running inference:  33%|███▎      | 26/79 [00:35<01:13,  1.39s/it]Running inference:  34%|███▍      | 27/79 [00:36<01:12,  1.39s/it]Running inference:  35%|███▌      | 28/79 [00:38<01:11,  1.40s/it]Running inference:  37%|███▋      | 29/79 [00:39<01:08,  1.38s/it]Running inference:  38%|███▊      | 30/79 [00:40<01:06,  1.36s/it]Running inference:  39%|███▉      | 31/79 [00:42<01:05,  1.36s/it]Running inference:  41%|████      | 32/79 [00:43<01:03,  1.35s/it]Running inference:  42%|████▏     | 33/79 [00:44<01:01,  1.34s/it]Running inference:  43%|████▎     | 34/79 [00:46<01:00,  1.34s/it]Running inference:  44%|████▍     | 35/79 [00:47<00:58,  1.33s/it]Running inference:  46%|████▌     | 36/79 [00:48<00:57,  1.34s/it]Running inference:  47%|████▋     | 37/79 [00:50<00:55,  1.33s/it]Running inference:  48%|████▊     | 38/79 [00:51<00:53,  1.31s/it]Running inference:  49%|████▉     | 39/79 [00:52<00:53,  1.33s/it]Running inference:  51%|█████     | 40/79 [00:54<00:52,  1.33s/it]Running inference:  52%|█████▏    | 41/79 [00:55<00:50,  1.33s/it]Running inference:  53%|█████▎    | 42/79 [00:56<00:49,  1.34s/it]Running inference:  54%|█████▍    | 43/79 [00:58<00:49,  1.37s/it]Running inference:  56%|█████▌    | 44/79 [00:59<00:47,  1.35s/it]Running inference:  57%|█████▋    | 45/79 [01:01<00:46,  1.36s/it]Running inference:  58%|█████▊    | 46/79 [01:02<00:44,  1.35s/it]Running inference:  59%|█████▉    | 47/79 [01:03<00:42,  1.33s/it]Running inference:  61%|██████    | 48/79 [01:05<00:41,  1.35s/it]Running inference:  62%|██████▏   | 49/79 [01:06<00:41,  1.37s/it]Running inference:  63%|██████▎   | 50/79 [01:08<00:41,  1.45s/it]Running inference:  65%|██████▍   | 51/79 [01:09<00:39,  1.41s/it]Running inference:  66%|██████▌   | 52/79 [01:10<00:37,  1.41s/it]Running inference:  67%|██████▋   | 53/79 [01:12<00:35,  1.37s/it]Running inference:  68%|██████▊   | 54/79 [01:13<00:34,  1.37s/it]Running inference:  70%|██████▉   | 55/79 [01:14<00:32,  1.35s/it]Running inference:  71%|███████   | 56/79 [01:16<00:30,  1.33s/it]Running inference:  72%|███████▏  | 57/79 [01:17<00:29,  1.33s/it]Running inference:  73%|███████▎  | 58/79 [01:18<00:27,  1.33s/it]Running inference:  75%|███████▍  | 59/79 [01:20<00:26,  1.33s/it]Running inference:  76%|███████▌  | 60/79 [01:21<00:25,  1.34s/it]Running inference:  77%|███████▋  | 61/79 [01:22<00:24,  1.34s/it]Running inference:  78%|███████▊  | 62/79 [01:24<00:22,  1.35s/it]Running inference:  80%|███████▉  | 63/79 [01:25<00:21,  1.35s/it]Running inference:  81%|████████  | 64/79 [01:26<00:20,  1.36s/it]Running inference:  82%|████████▏ | 65/79 [01:28<00:18,  1.34s/it]Running inference:  84%|████████▎ | 66/79 [01:29<00:17,  1.31s/it]Running inference:  85%|████████▍ | 67/79 [01:30<00:15,  1.30s/it]Running inference:  86%|████████▌ | 68/79 [01:31<00:14,  1.31s/it]Running inference:  87%|████████▋ | 69/79 [01:33<00:13,  1.31s/it]Running inference:  89%|████████▊ | 70/79 [01:34<00:11,  1.31s/it]Running inference:  90%|████████▉ | 71/79 [01:35<00:10,  1.32s/it]Running inference:  91%|█████████ | 72/79 [01:37<00:09,  1.31s/it]Running inference:  92%|█████████▏| 73/79 [01:38<00:07,  1.31s/it]Running inference:  94%|█████████▎| 74/79 [01:39<00:06,  1.33s/it]Running inference:  95%|█████████▍| 75/79 [01:41<00:05,  1.32s/it]Running inference:  96%|█████████▌| 76/79 [01:42<00:03,  1.32s/it]Running inference:  97%|█████████▋| 77/79 [01:43<00:02,  1.32s/it]Running inference:  99%|█████████▊| 78/79 [01:45<00:01,  1.32s/it]Running inference: 100%|██████████| 79/79 [01:45<00:00,  1.03it/s]Running inference: 100%|██████████| 79/79 [01:45<00:00,  1.33s/it]
01:16:19 - INFO - [inference_calibration] Duration: 105303.69ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
Running inference:   0%|          | 0/79 [00:00<?, ?it/s]Running inference:   1%|▏         | 1/79 [00:01<01:45,  1.35s/it]Running inference:   3%|▎         | 2/79 [00:02<01:43,  1.35s/it]Running inference:   4%|▍         | 3/79 [00:04<01:43,  1.36s/it]Running inference:   5%|▌         | 4/79 [00:05<01:40,  1.34s/it]Running inference:   6%|▋         | 5/79 [00:06<01:37,  1.31s/it]Running inference:   8%|▊         | 6/79 [00:07<01:35,  1.31s/it]Running inference:   9%|▉         | 7/79 [00:09<01:37,  1.35s/it]Running inference:  10%|█         | 8/79 [00:10<01:35,  1.34s/it]Running inference:  11%|█▏        | 9/79 [00:12<01:33,  1.34s/it]Running inference:  13%|█▎        | 10/79 [00:13<01:32,  1.34s/it]Running inference:  14%|█▍        | 11/79 [00:14<01:30,  1.34s/it]Running inference:  15%|█▌        | 12/79 [00:16<01:29,  1.34s/it]Running inference:  16%|█▋        | 13/79 [00:17<01:27,  1.32s/it]Running inference:  18%|█▊        | 14/79 [00:18<01:25,  1.32s/it]Running inference:  19%|█▉        | 15/79 [00:20<01:25,  1.33s/it]Running inference:  20%|██        | 16/79 [00:21<01:23,  1.33s/it]Running inference:  22%|██▏       | 17/79 [00:22<01:22,  1.32s/it]Running inference:  23%|██▎       | 18/79 [00:23<01:20,  1.33s/it]Running inference:  24%|██▍       | 19/79 [00:25<01:19,  1.33s/it]Running inference:  25%|██▌       | 20/79 [00:26<01:18,  1.32s/it]Running inference:  27%|██▋       | 21/79 [00:28<01:18,  1.35s/it]Running inference:  28%|██▊       | 22/79 [00:29<01:16,  1.34s/it]Running inference:  29%|██▉       | 23/79 [00:30<01:14,  1.34s/it]Running inference:  30%|███       | 24/79 [00:32<01:14,  1.35s/it]Running inference:  32%|███▏      | 25/79 [00:33<01:12,  1.35s/it]Running inference:  33%|███▎      | 26/79 [00:34<01:11,  1.35s/it]Running inference:  34%|███▍      | 27/79 [00:36<01:09,  1.34s/it]Running inference:  35%|███▌      | 28/79 [00:37<01:07,  1.33s/it]Running inference:  37%|███▋      | 29/79 [00:38<01:06,  1.34s/it]Running inference:  38%|███▊      | 30/79 [00:40<01:05,  1.33s/it]Running inference:  39%|███▉      | 31/79 [00:41<01:04,  1.35s/it]Running inference:  41%|████      | 32/79 [00:42<01:02,  1.34s/it]Running inference:  42%|████▏     | 33/79 [00:44<01:01,  1.34s/it]Running inference:  43%|████▎     | 34/79 [00:45<01:00,  1.33s/it]Running inference:  44%|████▍     | 35/79 [00:46<00:59,  1.35s/it]Running inference:  46%|████▌     | 36/79 [00:48<00:58,  1.37s/it]Running inference:  47%|████▋     | 37/79 [00:49<00:57,  1.37s/it]Running inference:  48%|████▊     | 38/79 [00:52<01:19,  1.94s/it]Running inference:  49%|████▉     | 39/79 [00:54<01:09,  1.75s/it]Running inference:  51%|█████     | 40/79 [00:55<01:03,  1.63s/it]Running inference:  52%|█████▏    | 41/79 [00:56<00:57,  1.52s/it]Running inference:  53%|█████▎    | 42/79 [00:58<00:54,  1.47s/it]Running inference:  54%|█████▍    | 43/79 [00:59<00:52,  1.45s/it]Running inference:  56%|█████▌    | 44/79 [01:00<00:49,  1.42s/it]Running inference:  57%|█████▋    | 45/79 [01:02<00:47,  1.40s/it]Running inference:  58%|█████▊    | 46/79 [01:03<00:45,  1.36s/it]Running inference:  59%|█████▉    | 47/79 [01:04<00:43,  1.37s/it]Running inference:  61%|██████    | 48/79 [01:06<00:42,  1.36s/it]Running inference:  62%|██████▏   | 49/79 [01:07<00:40,  1.34s/it]Running inference:  63%|██████▎   | 50/79 [01:08<00:39,  1.35s/it]Running inference:  65%|██████▍   | 51/79 [01:10<00:37,  1.35s/it]Running inference:  66%|██████▌   | 52/79 [01:11<00:36,  1.35s/it]Running inference:  67%|██████▋   | 53/79 [01:12<00:34,  1.34s/it]Running inference:  68%|██████▊   | 54/79 [01:14<00:33,  1.34s/it]Running inference:  70%|██████▉   | 55/79 [01:15<00:31,  1.33s/it]Running inference:  71%|███████   | 56/79 [01:16<00:31,  1.35s/it]Running inference:  72%|███████▏  | 57/79 [01:19<00:35,  1.60s/it]Running inference:  73%|███████▎  | 58/79 [01:20<00:32,  1.53s/it]Running inference:  75%|███████▍  | 59/79 [01:21<00:29,  1.47s/it]Running inference:  76%|███████▌  | 60/79 [01:23<00:27,  1.43s/it]Running inference:  77%|███████▋  | 61/79 [01:24<00:25,  1.40s/it]Running inference:  78%|███████▊  | 62/79 [01:25<00:23,  1.38s/it]Running inference:  80%|███████▉  | 63/79 [01:27<00:21,  1.36s/it]Running inference:  81%|████████  | 64/79 [01:28<00:20,  1.34s/it]Running inference:  82%|████████▏ | 65/79 [01:29<00:18,  1.36s/it]Running inference:  84%|████████▎ | 66/79 [01:31<00:17,  1.36s/it]Running inference:  85%|████████▍ | 67/79 [01:32<00:16,  1.34s/it]Running inference:  86%|████████▌ | 68/79 [01:33<00:14,  1.32s/it]Running inference:  87%|████████▋ | 69/79 [01:35<00:13,  1.32s/it]Running inference:  89%|████████▊ | 70/79 [01:36<00:11,  1.32s/it]Running inference:  90%|████████▉ | 71/79 [01:37<00:10,  1.34s/it]Running inference:  91%|█████████ | 72/79 [01:39<00:09,  1.33s/it]Running inference:  92%|█████████▏| 73/79 [01:40<00:07,  1.32s/it]Running inference:  94%|█████████▎| 74/79 [01:41<00:06,  1.32s/it]Running inference:  95%|█████████▍| 75/79 [01:43<00:05,  1.32s/it]Running inference:  96%|█████████▌| 76/79 [01:44<00:03,  1.32s/it]Running inference:  97%|█████████▋| 77/79 [01:45<00:02,  1.33s/it]Running inference:  99%|█████████▊| 78/79 [01:46<00:01,  1.32s/it]Running inference: 100%|██████████| 79/79 [01:47<00:00,  1.02it/s]Running inference: 100%|██████████| 79/79 [01:47<00:00,  1.36s/it]
01:18:06 - INFO - [inference_test] Duration: 107170.67ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
01:18:06 - INFO -     Inference: 212.48s for 10000 samples (47.06 samples/sec)
01:18:06 - INFO - Initialized ProbabilityExtractor
01:18:06 - INFO -   Temperature: 1.0
01:18:06 - INFO -   Calibration method: None
01:18:07 - INFO - [probability_extraction] Duration: 274.55ms | GPU Memory: 4788.3MB -> 4788.3MB (delta +0.0MB)
01:18:07 - INFO -     Raw probabilities saved to: outputs/results/probabilities/probs_gemma-2b-it_drs_float16_base_20251208_010140.npz
01:18:07 - INFO - Initialized LACScorer
01:18:07 - INFO -   Alpha: 0.1
01:18:07 - INFO -   Target coverage: 90.0%
01:18:07 - INFO - Initialized LAC (Least Ambiguous set-valued Classifiers) scorer
01:18:07 - INFO - Initialized PredictionSetGenerator
01:18:07 - INFO -   Methods: ['lac']
01:18:07 - INFO -   Alpha: 0.1
01:18:07 - INFO -   Aggregation: separate
01:18:07 - INFO - Calibrating 1 conformal predictors...
01:18:07 - INFO -   Calibrating LAC...
01:18:07 - INFO - Calibrating with 4999 samples...
01:18:07 - INFO - Calibration complete
01:18:07 - INFO -   Threshold: 0.9966
01:18:07 - INFO -   Score range: [0.0000, 1.0000]
01:18:07 - INFO - Calibration complete
01:18:07 - INFO - Generating prediction sets using LAC...
01:18:07 - INFO - Generating prediction sets for 5001 test instances...
01:18:07 - INFO - Prediction complete
01:18:07 - INFO -   Average set size: 5.26
01:18:07 - INFO -   Coverage rate: 90.24%
01:18:07 - INFO -   Meets coverage guarantee: True
01:18:07 - INFO - [PASS] Coverage guarantee met: 90.24% >= 90.00%
01:18:07 - INFO -     LAC: Acc=22.80%, CR=90.24%, SS=5.26
01:18:07 - INFO - Initialized APSScorer
01:18:07 - INFO -   Alpha: 0.1
01:18:07 - INFO -   Target coverage: 90.0%
01:18:07 - INFO - Initialized APS (Adaptive Prediction Sets) scorer
01:18:07 - INFO - Initialized PredictionSetGenerator
01:18:07 - INFO -   Methods: ['aps']
01:18:07 - INFO -   Alpha: 0.1
01:18:07 - INFO -   Aggregation: separate
01:18:07 - INFO - Calibrating 1 conformal predictors...
01:18:07 - INFO -   Calibrating APS...
01:18:07 - INFO - Calibrating with 4999 samples...
01:18:07 - INFO - Calibration complete
01:18:07 - INFO -   Threshold: 1.0000
01:18:07 - INFO -   Score range: [0.2331, 1.0000]
01:18:07 - INFO - Calibration complete
01:18:07 - INFO - Generating prediction sets using APS...
01:18:07 - INFO - Generating prediction sets for 5001 test instances...
01:18:07 - INFO - Prediction complete
01:18:07 - INFO -   Average set size: 5.16
01:18:07 - INFO -   Coverage rate: 90.80%
01:18:07 - INFO -   Meets coverage guarantee: True
01:18:07 - INFO - [PASS] Coverage guarantee met: 90.80% >= 90.00%
01:18:07 - INFO -     APS: Acc=22.80%, CR=90.80%, SS=5.16
01:18:11 - INFO - Unloaded model: gemma-2b-it_float16
01:18:12 - INFO - Checkpoint saved after completing: gemma-2b-it | drs | float16
01:18:12 - INFO - 
================================================================================
01:18:12 - INFO - Run 5/5: gemma-2b-it | ds | float16
01:18:12 - INFO - ================================================================================
01:18:12 - INFO - [start_gemma-2b-it_ds] GPU State: Allocated: 8.1MB | Reserved: 4780.0MB | Free: 81147.6MB | Utilization: 100.0%
01:18:12 - INFO - Loading ds dataset (10000 samples)...
01:18:12 - INFO - Loading HaluSum dataset with 10000 samples...
01:18:13 - INFO - Loaded 10000 HaluSum instances
01:18:13 - INFO - [dataset_loading] Duration: 1745.87ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
01:18:13 - INFO - Processing ds dataset to 6-option format...
01:18:49 - INFO - Processed 10000 instances for ds
01:18:49 - INFO - Option expansion statistics for ds:
01:18:49 - INFO -   Instances expanded (2→4 options): 10000
01:18:49 - INFO -   Total options sampled: 20000
01:18:49 - INFO -   Duplicate options avoided: 0
01:18:49 - INFO -   Fallback options used: 0
01:18:49 - INFO - Splitting ds dataset (calibration: 50%, test: 50%)
01:18:49 - INFO - Split complete:
01:18:49 - INFO -   Calibration: 4999 instances
01:18:49 - INFO -   Test: 5001 instances
01:18:49 - INFO - Answer distribution:
01:18:49 - INFO -   Calibration: {'A': 1258, 'B': 1203, 'C': 1264, 'D': 1274}
01:18:49 - INFO -   Test: {'A': 1258, 'B': 1204, 'C': 1265, 'D': 1274}
01:18:49 - INFO - [dataset_processing] Duration: 35766.75ms | GPU Memory: 8.1MB -> 8.1MB (delta +0.0MB)
01:18:49 - INFO - Initialized DemonstrationSelector
01:18:49 - INFO -   Strategy: random
01:18:49 - INFO -   Num demonstrations: 5
01:18:49 - INFO - Initialized DemonstrationManager
01:18:49 - INFO - Selecting 1 demonstrations using 'random' strategy
01:18:49 - INFO - Selected and cached 1 demonstrations for ds
01:18:49 - INFO - Loading model: gemma-2b-it (float16)
01:18:49 - INFO - Loading model: gemma-2b-it_float16 (google/gemma-2b-it)
01:18:49 - INFO - Loading tokenizer from google/gemma-2b-it
01:18:50 - INFO - Tokenizer loaded successfully
01:18:50 - INFO -   Vocab size: 256000
01:18:50 - INFO -   Padding side: left
01:18:50 - INFO -   PAD token: <pad> (ID: 0)
01:18:50 - INFO - Loading model from google/gemma-2b-it
01:18:50 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.67it/s]
01:18:52 - INFO - Model loaded successfully
01:18:52 - INFO -   GPU Memory: 4.68GB allocated, 4.77GB reserved

Model: gemma-2b-it_float16
  ID: google/gemma-2b-it
  Type: causal
  Parameters: 2,506,172,416
  Vocab size: 256,000
  Max length: 8192
  Device: cuda:0
  Dtype: torch.float16
  Instruct-tuned: True
01:18:52 - INFO - [model_loading] Duration: 2758.07ms | GPU Memory: 8.1MB -> 4788.3MB (delta +4780.1MB)
01:18:52 - INFO - [after_model_load_gemma-2b-it] GPU State: Allocated: 4788.3MB | Reserved: 4882.0MB | Free: 76367.5MB | Utilization: 2.0%
01:18:52 - INFO - Using batch size: 64
01:18:52 - INFO -   Strategy: base
01:18:52 - INFO - Initialized PromptBuilder for task: ds
01:18:52 - INFO -   Available strategies: ['base', 'shared_instruction', 'task_specific']
01:18:52 - INFO - Building 10000 prompts using 'base' strategy with 1 demonstrations
01:18:52 - INFO - Initialized InferenceEngine for gemma-2b-it_float16
01:18:52 - INFO -   Device: cuda:0
01:18:52 - INFO -   Batch size: 64
01:18:52 - INFO -   Option tokens: {'A': 586, 'B': 599, 'C': 585, 'D': 608, 'E': 637, 'F': 633}
Running inference:   0%|          | 0/79 [00:00<?, ?it/s]Running inference:   1%|▏         | 1/79 [00:05<06:35,  5.07s/it]Running inference:   1%|▏         | 1/79 [00:09<11:44,  9.03s/it]
01:19:01 - INFO - [inference_calibration] Duration: 9031.99ms | GPU Memory: 4788.3MB -> 5302.3MB (delta +514.0MB)
01:19:01 - ERROR - Failed: gemma-2b-it | ds | float16
01:19:01 - ERROR - Error: CUDA out of memory. Tried to allocate 62.50 GiB. GPU 0 has a total capacity of 79.25 GiB of which 11.48 GiB is free. Process 1776638 has 67.77 GiB memory in use. Of the allocated memory 5.18 GiB is allocated by PyTorch, and 62.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/BLUQ/run_benchmark.py", line 337, in run
    results = self._run_single_configuration(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/run_benchmark.py", line 535, in _run_single_configuration
    cal_results = inference_engine.infer_batch(cal_prompts, show_progress=True)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/src/models/inference_engine.py", line 392, in infer_batch
    batch_results = self._infer_batch_internal(batch_prompts, batch_ids)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/src/models/inference_engine.py", line 434, in _infer_batch_internal
    outputs = self.model(**inputs, use_cache=use_cache)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/transformers/models/gemma/modeling_gemma.py", line 482, in forward
    logits = self.lm_head(hidden_states[:, slice_indices, :])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/BLUQ/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 62.50 GiB. GPU 0 has a total capacity of 79.25 GiB of which 11.48 GiB is free. Process 1776638 has 67.77 GiB memory in use. Of the allocated memory 5.18 GiB is allocated by PyTorch, and 62.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
01:19:01 - INFO - Generating all visualizations...
01:19:02 - INFO - Saved heatmap to outputs/results/figures/heatmap_accuracy.png
01:19:02 - INFO - Saved heatmap to outputs/results/figures/heatmap_coverage_rate.png
01:19:03 - INFO - Saved heatmap to outputs/results/figures/heatmap_avg_set_size.png
01:19:04 - INFO - Saved dashboard to outputs/results/figures/dashboard.png
01:19:05 - INFO - Saved bar chart to outputs/results/figures/bar_comparison_accuracy.png
01:19:05 - INFO - Saved bar chart to outputs/results/figures/bar_comparison_avg_set_size.png
01:19:06 - INFO - Saved radar chart to outputs/results/figures/radar_chart.png
01:19:06 - INFO - Saved uncertainty analysis to outputs/results/figures/uncertainty_analysis.png
01:19:06 - INFO - Saved summary table to outputs/results/figures/results_summary.md
01:19:06 - INFO - Saved summary table to outputs/results/figures/results_summary_lac.md
01:19:06 - INFO - Saved summary table to outputs/results/figures/results_summary_aps.md
01:19:06 - INFO - All visualizations saved to outputs/results/figures
01:19:06 - INFO - Visualizations saved to: outputs/results/figures
01:19:07 - INFO - Stopped GPU monitoring. Collected 462 snapshots.

================================================================================
GPU PROFILING SUMMARY
================================================================================

Total Operations: 26
Total Time: 1018425.65ms (1018.43s)

--- Operations Breakdown ---

  [inference_calibration]
    Count: 5
    Total: 463766.19ms (45.5%)
    Avg: 92753.24ms | Min: 9031.99ms | Max: 168867.27ms
    Avg Memory Delta: +176.2MB

  [inference_test]
    Count: 3
    Total: 446807.91ms (43.9%)
    Avg: 148935.97ms | Min: 107170.67ms | Max: 170285.22ms
    Avg Memory Delta: +0.0MB

  [dataset_processing]
    Count: 5
    Total: 69693.79ms (6.8%)
    Avg: 13938.76ms | Min: 36.28ms | Max: 35766.75ms
    Avg Memory Delta: +0.0MB

  [model_loading]
    Count: 5
    Total: 22347.12ms (2.2%)
    Avg: 4469.42ms | Min: 2525.51ms | Max: 11924.50ms
    Avg Memory Delta: +4780.1MB

  [dataset_loading]
    Count: 5
    Total: 14969.53ms (1.5%)
    Avg: 2993.91ms | Min: 1745.87ms | Max: 4489.94ms
    Avg Memory Delta: +0.0MB

  [probability_extraction]
    Count: 3
    Total: 841.12ms (0.1%)
    Avg: 280.37ms | Min: 273.99ms | Max: 292.58ms
    Avg Memory Delta: +0.0MB

--- Memory Statistics ---
  Peak Allocated: 69302.3MB
  Avg Allocated: 28943.5MB
  Avg GPU Utilization: 86.7%

--- Bottlenecks & Recommendations ---

  [WARN] inference_calibration (45.5% of total time)
     -> Inference is slow. Consider: larger batch size, Flash Attention, or quantization.

  [WARN] inference_test (43.9% of total time)
     -> Inference is slow. Consider: larger batch size, Flash Attention, or quantization.

================================================================================
01:19:07 - INFO - Saved GPU profiling report to outputs/results/gpu_profile_20251208_010140.json
01:19:07 - INFO - GPU profiling report saved to: outputs/results/gpu_profile_20251208_010140.json
01:19:07 - INFO - 
================================================================================
01:19:07 - INFO - BENCHMARK COMPLETE
01:19:07 - INFO - ================================================================================
01:19:07 - INFO - Total time: 17.32 minutes
01:19:07 - INFO - Results saved to: outputs/results

================================================================================
FINAL SUMMARY
================================================================================
Total runs: 6
Overall accuracy: 22.58%
Overall coverage: 91.85%
Overall set size: 5.34
Guarantee met: 100.00%
Total time: 17.32 minutes
Log file: outputs/results/logs/benchmark_20251208_010140.log
================================================================================

01:19:07 - INFO - Benchmark completed successfully
01:19:07 - INFO - Total runs: 6
01:19:07 - INFO - Overall accuracy: 22.58%
01:19:07 - INFO - Overall coverage: 91.85%
01:19:07 - INFO - Log file saved to: outputs/results/logs/benchmark_20251208_010140.log
