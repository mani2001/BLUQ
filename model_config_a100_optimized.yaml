# Optimized Model Configuration for NVIDIA A100 80GB
# This configuration maximizes inference speed by:
# 1. Using larger batch sizes (80GB memory allows this)
# 2. Enabling FP16 precision for faster computation
# 3. Optimizing cache usage
# 4. Selecting faster models first

cache_dir: "./models/cache"
output_dir: "./results/models"
save_logits: false  # Disable to save disk I/O time
save_probabilities: true

models:
  # Tier 1: Smallest/Fastest models (135M - 450M parameters)
  # These can use very large batch sizes
  smollm-135m:
    name: "smollm-135m"
    model_id: "HuggingFaceTB/SmolLM-135M"
    description: "Smallest model - fastest inference"
    tags: ["smollm", "base", "ultra-fast"]
    
    load_config:
      model_id: "HuggingFaceTB/SmolLM-135M"
      dtype: "float16"  # FP16 for speed
      device: "cuda"    # Force CUDA
      trust_remote_code: false
    
    inference_config:
      batch_size: 16   # Reduced for memory safety with long prompts
      max_length: 2048
      device: "cuda"
      use_fp16: true
      use_cache: true
      temperature: 1.0
      extract_last_token_only: true
    
    probability_config:
      temperature: 1.0
      normalize: true
      calibration_method: null

  smollm-360m:
    name: "smollm-360m"
    model_id: "HuggingFaceTB/SmolLM-360M"
    description: "Small model - very fast inference"
    tags: ["smollm", "base", "fast"]
    
    load_config:
      model_id: "HuggingFaceTB/SmolLM-360M"
      dtype: "float16"
      device: "cuda"
      trust_remote_code: false
    
    inference_config:
      batch_size: 8
      max_length: 2048
      device: "cuda"
      use_fp16: true
      use_cache: true
      temperature: 1.0
      extract_last_token_only: true
    
    probability_config:
      temperature: 1.0
      normalize: true
      calibration_method: null

  openelm-270m:
    name: "openelm-270m"
    model_id: "apple/OpenELM-270M"
    description: "Apple's efficient language model"
    tags: ["openelm", "base", "fast"]
    
    load_config:
      model_id: "apple/OpenELM-270M"
      dtype: "float16"
      device: "cuda"
      trust_remote_code: true
    
    inference_config:
      batch_size: 96
      max_length: 2048
      device: "cuda"
      use_fp16: true
      use_cache: true
      temperature: 1.0
      extract_last_token_only: true
    
    probability_config:
      temperature: 1.0
      normalize: true
      calibration_method: null

  # Tier 2: Medium models (1.1B - 1.8B parameters)
  # Moderate batch sizes
  tinyllama-1.1b:
    name: "tinyllama-1.1b"
    model_id: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    description: "TinyLlama instruct model"
    tags: ["tinyllama", "instruct", "medium"]
    
    load_config:
      model_id: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
      dtype: "float16"
      device: "cuda"
      trust_remote_code: false
    
    inference_config:
      batch_size: 8
      max_length: 2048
      device: "cuda"
      use_fp16: true
      use_cache: true
      temperature: 1.0
      extract_last_token_only: true
    
    probability_config:
      temperature: 1.0
      normalize: true
      calibration_method: null

  smollm-1.7b:
    name: "smollm-1.7b"
    model_id: "HuggingFaceTB/SmolLM-1.7B"
    description: "Largest SmolLM model"
    tags: ["smollm", "base", "medium"]
    
    load_config:
      model_id: "HuggingFaceTB/SmolLM-1.7B"
      dtype: "float16"
      device: "cuda"
      trust_remote_code: false
    
    inference_config:
      batch_size: 24
      max_length: 2048
      device: "cuda"
      use_fp16: true
      use_cache: true
      temperature: 1.0
      extract_last_token_only: true
    
    probability_config:
      temperature: 1.0
      normalize: true
      calibration_method: null

  qwen-1.8b:
    name: "qwen-1.8b"
    model_id: "Qwen/Qwen-1_8B"
    description: "Qwen base model"
    tags: ["qwen", "base", "medium"]
    
    load_config:
      model_id: "Qwen/Qwen-1_8B"
      dtype: "float16"
      device: "cuda"
      trust_remote_code: true
    
    inference_config:
      batch_size: 24
      max_length: 2048
      device: "cuda"
      use_fp16: true
      use_cache: true
      temperature: 1.0
      extract_last_token_only: true
    
    probability_config:
      temperature: 1.0
      normalize: true
      calibration_method: null

  # Tier 3: Larger models (2B - 2.7B parameters)
  # Smaller batch sizes but still optimized
  phi-2:
    name: "phi-2"
    model_id: "microsoft/phi-2"
    description: "Microsoft Phi-2 - high quality small model"
    tags: ["phi", "base", "large"]
    
    load_config:
      model_id: "microsoft/phi-2"
      dtype: "float16"
      device: "cuda"
      trust_remote_code: true
    
    inference_config:
      batch_size: 16
      max_length: 2048
      device: "cuda"
      use_fp16: true
      use_cache: true
      temperature: 1.0
      extract_last_token_only: true
    
    probability_config:
      temperature: 1.0
      normalize: true
      calibration_method: null

  gemma-2b:
    name: "gemma-2b"
    model_id: "google/gemma-2b"
    description: "Google Gemma base model"
    tags: ["gemma", "base", "large"]
    
    load_config:
      model_id: "google/gemma-2b"
      dtype: "float16"
      device: "cuda"
      trust_remote_code: false
    
    inference_config:
      batch_size: 16
      max_length: 2048
      device: "cuda"
      use_fp16: true
      use_cache: true
      temperature: 1.0
      extract_last_token_only: true
    
    probability_config:
      temperature: 1.0
      normalize: true
      calibration_method: null

  gemma-2b-it:
    name: "gemma-2b-it"
    model_id: "google/gemma-2b-it"
    description: "Google Gemma instruct model"
    tags: ["gemma", "instruct", "large"]
    
    load_config:
      model_id: "google/gemma-2b-it"
      dtype: "float16"
      device: "cuda"
      trust_remote_code: false
    
    inference_config:
      batch_size: 16
      max_length: 2048
      device: "cuda"
      use_fp16: true
      use_cache: true
      temperature: 1.0
      extract_last_token_only: true
    
    probability_config:
      temperature: 1.0
      normalize: true
      calibration_method: null
